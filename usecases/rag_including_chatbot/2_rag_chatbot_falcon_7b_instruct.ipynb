{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486225e8-12a5-499f-a620-ea41ccef1815",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation and Chatbot Application\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. The key aspects of this framework allow us to augement the Large Models and enable us to perform tasks which meet our goals and enable our use-cases. At a high level Langchain has \n",
    "\n",
    "Data: Connect a language model to other sources of data\n",
    "Agent: Allow a language model to interact with its environment\n",
    "\n",
    "LangChain can be used in two major ways:\n",
    "\n",
    "<li>Indivisual Components: LangChain provides modular abstractions for the components neccessary to work with language models. LangChain also has collections of implementations for all these abstractions. The components are designed to be easy to use, regardless of whether you are using the rest of the LangChain framework or not.\n",
    "\n",
    "<li>Use-Case Specific Chains: Chains can be thought of as assembling these components in particular ways in order to best accomplish a particular use case. These are intended to be a higher level interface through which people can easily get started with a specific use case. These chains are also designed to be customizable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4789df3c-aff9-4957-a32d-80086b1f7ddb",
   "metadata": {},
   "source": [
    "## Topics covered:\n",
    "\n",
    "In this notebook we will be covering the below topics:\n",
    "\n",
    "- **LLM** Examine running an LLM in bare form to check for output\n",
    "- **Vector DB** Examine various vector databases like FAISS or CHROMA and leverage to produce better results using RAG\n",
    "- **Prompt template** Examine use of PROMPT Template\n",
    "- **Question Answering** Retrieval Augmented Generation (RAG)\n",
    "- **Chatbot** Build a Interactive Chatbot with Memory "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1176e9-9a60-4713-b72f-9e54d2a259b8",
   "metadata": {},
   "source": [
    "## Key points for consideration\n",
    "\n",
    "1. Long Document that exceed the token limit? Ability to Chain , Mapo_reduce, Refine, Map-Rerank\n",
    "2. Cost of per token -- minimize the tokens and send in only relevant tokens to Model\n",
    "3. Which model to use --\n",
    "    - Cohere, AI21, Huggingface Hub, Manifest, Goose AI, Writer, Banana, Modal, StochasticAI, Cerebrium, Petals, Forefront AI, Anthropic, DeepInfra, and self-hosted Models.\n",
    "    - Example LLM cohere = Cohere(model='command-xlarge')\n",
    "    - Example LLM flan = HuggingFaceHub(repo_id=\"google/flan-t5-xl\")\n",
    "4. Input Data Sources PDF, WebPages, CSV , S3, EFS\n",
    "5. Orchestration with External Tasks\n",
    "    - External Tasks - Agent SerpApi, SEARCH Engines\n",
    "    - Math Calculator\n",
    "6. Conversation Management and History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de785d0-3b27-4699-87be-a34484c429fa",
   "metadata": {},
   "source": [
    "### Key components of LangChain\n",
    "\n",
    "Let us examine the key components of Langchain. At the heart and the center is the Large Model.\n",
    "\n",
    "There are several main modules that LangChain provides support for. For each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides. These modules are, in increasing order of complexity:\n",
    "\n",
    "**Models**: The various model types and model integrations LangChain supports.\n",
    "\n",
    "<img src='./images/models.png' width =\"500\"/>\n",
    "\n",
    "    \n",
    "**Prompts**: This includes prompt management, prompt optimization, and prompt serialization.\n",
    "    \n",
    "<img src=\"images/prompt.png\" width=\"500\"/>\n",
    "    \n",
    "**Memory**: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\n",
    "\n",
    "    \n",
    "**Indexes**: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\n",
    "    \n",
    "<img src=\"images/vectorstore.png\" width=\"500\"/>\n",
    "\n",
    "**Chains**: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\n",
    "\n",
    "<img src=\"images/chains.png\" width=\"500\"/>\n",
    "\n",
    "**Agents**: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\n",
    "\n",
    "\n",
    "    \n",
    "**Callbacks**: It can be difficult to track all that occurs inside a chain or agent. Callbacks help add a level of observability and introspection.\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402714bf-14b6-4481-8e33-fc3d0b8a81f4",
   "metadata": {},
   "source": [
    "### Chat Bot key elements\n",
    "\n",
    "The first process in a chat bot is to generate embeddings. Typically you will have an ingestion process which will run through your embedding model and generate the embeddings which will be stored in a sort of a vector store. In this example we are using a GPT-J embeddings model for this\n",
    "\n",
    "<img src=\"images/Embeddings_lang.png\" width=\"600\"/>\n",
    "\n",
    "Second process is the user request orchestration , interaction,  invoking and returing the results\n",
    "\n",
    "<img src=\"images/Chatbot_lang.png\" width=\"600\"/>\n",
    "\n",
    "For processes which need deeper analysis, conversation history we will need to summarize every interaction to keep it succinct and for that we can follow this flow below which uses PineCone as an example\n",
    "\n",
    "For the various Tools which are available \n",
    "\n",
    "<img src=\"images/chatbot_internet.jpg\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed6880-101b-457a-9e99-25cc421ee8c5",
   "metadata": {},
   "source": [
    "# Pre-Requisites\n",
    "\n",
    "There are a few pre-reqs to be completed when running this notebook. The key one being setting up the LLM to be used.\n",
    "- Either have a Llama-2-7b model deployed in SageMaker\n",
    "- Have Anthropic Model Key . You can choose to do both or either or . \n",
    "    \n",
    "However certains cells might not work if you have just 1 and so you can choose to ignore those errors as part of the run\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd17ed3-981c-4943-b7be-f218c6ce6f6c",
   "metadata": {},
   "source": [
    "# Deploy Falcon-7b instruct using Jumpstart\n",
    "\n",
    "In this section we will deploy a Falcon-7b instruct model using just a few simple steps with Amazon SageMaker Jumpstart. Let's install some dependencies first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6e632c30-db64-4e60-9ce3-8fbcfb798e7e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: amazon-textract-caller in /opt/conda/lib/python3.10/site-packages (0.0.29)\n",
      "Collecting pypdf\n",
      "  Obtaining dependency information for pypdf from https://files.pythonhosted.org/packages/bf/53/8840f93c5dcd108c02cac7343e194f9dc5d15ade6200ccc661ab4e1352b5/pypdf-3.16.2-py3-none-any.whl.metadata\n",
      "  Downloading pypdf-3.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (9.4.0)\n",
      "Requirement already satisfied: boto3>=1.26.35 in /opt/conda/lib/python3.10/site-packages (from amazon-textract-caller) (1.26.132)\n",
      "Requirement already satisfied: botocore in /opt/conda/lib/python3.10/site-packages (from amazon-textract-caller) (1.29.132)\n",
      "Requirement already satisfied: amazon-textract-response-parser>=0.1.39 in /opt/conda/lib/python3.10/site-packages (from amazon-textract-caller) (1.0.1)\n",
      "Requirement already satisfied: marshmallow<4,>=3.14 in /opt/conda/lib/python3.10/site-packages (from amazon-textract-response-parser>=0.1.39->amazon-textract-caller) (3.20.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.26.35->amazon-textract-caller) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.26.35->amazon-textract-caller) (0.6.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore->amazon-textract-caller) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore->amazon-textract-caller) (1.26.15)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4,>=3.14->amazon-textract-response-parser>=0.1.39->amazon-textract-caller) (23.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore->amazon-textract-caller) (1.16.0)\n",
      "Downloading pypdf-3.16.2-py3-none-any.whl (276 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.3/276.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-3.16.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade sagemaker --quiet\n",
    "# !pip install --upgrade langchain --quiet\n",
    "# !pip install --upgrade transformers sentence-transformers --quiet\n",
    "# !pip install faiss-cpu==1.7.4 --quiet\n",
    "!pip install amazon-textract-caller pypdf Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "a408e2be-6b82-46d1-a066-5c4ef785c0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "Region is us-east-1, Role is arn:aws:iam::965425568475:role/service-role/A2ISageMaker-ExecutionRole-20220304T091651\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "print(f\"Region is {aws_region}, Role is {aws_role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "589bc009-dd0b-4839-95bd-5a9907a84887",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Available Llama-2 Models =====\n",
      "huggingface-llm-bilingual-rinna-4b-instruction-ppo-bf16\n",
      "huggingface-llm-falcon-180b-bf16\n",
      "huggingface-llm-falcon-180b-chat-bf16\n",
      "huggingface-llm-falcon-40b-bf16\n",
      "huggingface-llm-falcon-40b-instruct-bf16\n",
      "huggingface-llm-falcon-7b-bf16\n",
      "huggingface-llm-falcon-7b-instruct-bf16\n",
      "huggingface-llm-rinna-3-6b-instruction-ppo-bf16\n"
     ]
    }
   ],
   "source": [
    "# To list all the available textgeneration models in JumpStart uncomment and run the code below\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "filter_value = \"task == llm\"\n",
    "\n",
    "print(\"===== Available Llama-2 Models =====\")\n",
    "text_generation_models = list_jumpstart_models(filter=filter_value)\n",
    "for model in text_generation_models:\n",
    "        print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "18101583-5b29-4f1e-8d41-8777472f4d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = 'huggingface-llm-falcon-40b-instruct-bf16'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f27e1-140e-4c80-b94f-6895cdc17f27",
   "metadata": {},
   "source": [
    "We will now deploy this model to a SageMaker endpoint for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "e264081d-7afc-47a4-bca6-9e9140bc7e60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "try:\n",
    "    model = JumpStartModel(model_id=model_id, instance_type=\"ml.g5.12xlarge\")\n",
    "    predictor = model.deploy()\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "18cb4b4e-02cf-40f0-b63e-19c53c52259b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "region = aws_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "78e3808d-3f10-4b14-a31e-4eaea817d54c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Endpoint with Falcon-7b instruct deployed: hf-llm-falcon-40b-instruct-bf16-2023-09-28-18-42-22-097\n"
     ]
    }
   ],
   "source": [
    "print(f\"SageMaker Endpoint with Falcon-7b instruct deployed: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e551a45e-0893-46e2-b278-fd3181112df7",
   "metadata": {},
   "source": [
    "## Simple Q&A with Falcon \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d70d943-b373-4426-ae05-c0d866068a74",
   "metadata": {},
   "source": [
    "In order to use our model endpoint with LangChain we wrap up endpoints for LLM into `langchain.llms.sagemaker_endpoint.SagemakerEndpoint` which is LangChain's built in support for SageMaker endpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "142bb7d5-dd26-4a48-ad2c-4295587e418f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from langchain import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "    \n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt,  \"parameters\": model_kwargs}) \n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "sm_llm=SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name, \n",
    "        region_name=aws_region,\n",
    "        model_kwargs={\"do_sample\": True,\n",
    "                                    \"top_p\": 0.9,\n",
    "                                    \"temperature\": 0.5,\n",
    "                                    \"max_new_tokens\":  200,\n",
    "                                    \"stop\": [\"<|endoftext|>\", \"</s>\"]},\n",
    "        content_handler=content_handler\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae41aee2-3b22-4ef8-8886-1686515effd8",
   "metadata": {},
   "source": [
    "Next, we will use LangChain PromptTemplate and LLMChain to create a prompt and invoke the model endpoint to get a response. We will use this method, or methods similar to this using LangChain throughout the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "58438587-604f-4163-8f1f-dd977dc94524",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Expanse is a science fiction television series based on the book series of the same name by Daniel Abraham and Ty Franck, under the pen name James S. A. Corey. The series follows a group of characters who navigate the political and social landscape of a future in which humanity has colonized the solar system. The plot revolves around a conspiracy that threatens the stability of the system and the survival of the human race.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# define the prompt template\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# define an LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=sm_llm)\n",
    "\n",
    "# Run the chain\n",
    "output = llm_chain.run(question=  \"What is the plot of 'The Expanse'?\", stop=[\"Question:\",\"\\n\"])\n",
    "\n",
    "print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ac293-d6a6-4e61-ad6f-18a10380d6bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contextual Q&A with Falcon-7b instruct\n",
    "---\n",
    "\n",
    "Given a context, ask Falcon-7b instruct to answer only from within that context. Let's create a prompt template for that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "4049f468-675b-4191-be30-4a592b14a609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful assistant. Given a document, answer the 'Question'. Keep your answers strictly from within the document. \n",
    "If the answer to the question is not in the document, simplay say \"I do not know\", do not make up an answer.\n",
    "\n",
    "Document: {document}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# define the prompt template\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"document\",\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "ec5e0998-deb9-4a2d-970c-bf26bf324cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first season of 'The Expanse' premiered on December 14, 2015.\n"
     ]
    }
   ],
   "source": [
    "document=\"\"\"The Expanse is a science fiction television series based on the novel series of the same name by James S. A. Corey (Daniel Abraham and Ty Franck). \\\n",
    "It was developed by Mark Fergus and Hawk Ostby, who served as executive producers alongside Naren Shankar, Andrew Kosove, Broderick Johnson, Laura Lancaster, \\\n",
    "Sean Daniel, Jason Brown, and Sharon Hall. The first season premiered on December 14, 2015, with the second season following on February 1, 2017. The third \\\n",
    "season premiered on April 11, 2018.\n",
    "\"\"\"\n",
    "\n",
    "# define an LLMChain\n",
    "llm_chain = LLMChain(prompt=qa_prompt, llm=sm_llm)\n",
    "\n",
    "# Run the chain\n",
    "output = llm_chain.run(document=document, question=\"When did the first season of 'The Expanse' premier?\", stop=[\"Question:\",\"\\n\"])\n",
    "\n",
    "print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb224a-3419-42dc-afd1-b3aa1bb26210",
   "metadata": {},
   "source": [
    "### Let's ask it something completely outside of the document\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "49386ce8-ea15-4804-ab33-6b876368d591",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking Bad is an American television series created and produced by Vince Gilligan. It premiered on January 20, 2008, and concluded on September 29, 2013, consisting of five seasons for a total of 62 episodes.\n"
     ]
    }
   ],
   "source": [
    "document=\"\"\"The Expanse is a science fiction television series based on the novel series of the same name by James S. A. Corey (Daniel Abraham and Ty Franck). \\\n",
    "It was developed by Mark Fergus and Hawk Ostby, who served as executive producers alongside Naren Shankar, Andrew Kosove, Broderick Johnson, Laura Lancaster, \\\n",
    "Sean Daniel, Jason Brown, and Sharon Hall. The first season premiered on December 14, 2015, with the second season following on February 1, 2017. The third \\\n",
    "season premiered on April 11, 2018.\n",
    "\"\"\"\n",
    "\n",
    "# define an LLMChain\n",
    "llm_chain = LLMChain(prompt=qa_prompt, llm=sm_llm)\n",
    "\n",
    "# Run the chain\n",
    "output = llm_chain.run(document=document, question=\"When was 'Breaking Bad' made?\", stop=[\"Question:\",\"\\n\"])\n",
    "\n",
    "print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1251cf-d0ed-4c15-933e-7f67abfbe6ff",
   "metadata": {},
   "source": [
    "We may see the model respond with an answer, which may be correct afterall, but the context doesn't include any details about the question asked. We can mitigate this with few shot prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de1934-6133-4d1d-beaf-0c142eb65a62",
   "metadata": {},
   "source": [
    "## Few shot Q&A\n",
    "---\n",
    "\n",
    "In this section we will perform \"few shot\" Q&A with the model. We will show it a few example and then ask it a question to be answered based on a given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "f22dd602-e373-4e63-9eb5-2dc07db0e01d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, say \"I don't know\"\n",
    "\n",
    "Context: Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote \n",
    "what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, \n",
    "just characters with strong feelings, which I imagined made them deep.\n",
    "Question: What are the two things the author worked outside of school?\n",
    "Answer: Writing and programming\n",
    "===\n",
    "Context: The prevalence of malnutrition among elementary school aged children in tehran varied from 6% to 16% .\n",
    "Anthropometric study of elementary school students in shiraz revealed that 16% of them suffer from malnutrition and low body weight .\n",
    "Question: What steps did the ministry of education take to address the issue?\n",
    "Answer: I do not know\n",
    "===\n",
    "Context: {document}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# define the prompt template\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"document\",\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "6f20d16b-6cc6-4ce2-bca2-16c3475e1a46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I cannot find any information about Ms. Agatha's college education in the given document.\n"
     ]
    }
   ],
   "source": [
    "document=\"\"\"In the quiet town of Willowbrook, elderly Ms. Agatha discovered a mysterious old key while tending to her garden. \n",
    "Curiosity piqued, she recalled an ancient wooden chest in her attic, untouched for decades. Climbing the creaky steps, she unlocked the chest \n",
    "to reveal a collection of letters penned by her grandmother. These letters unveiled stories of a hidden world filled with magical creatures and \n",
    "enchanted forests. As she read, the wind outside picked up, carrying whispers of the adventures her ancestors had once embarked upon. Willowbrook, \n",
    "it seemed, was not as ordinary as she had always believed.\n",
    "\"\"\"\n",
    "\n",
    "# define an LLMChain\n",
    "llm_chain = LLMChain(prompt=qa_prompt, llm=sm_llm)\n",
    "\n",
    "# Run the chain\n",
    "output = llm_chain.run(document=document, question=\"Where did Ms. Agatha went to college?\", stop=[\"Question:\",\"\\n\"])\n",
    "\n",
    "print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d94d7-e413-400e-863b-d4ad2bce19f9",
   "metadata": {},
   "source": [
    "Now, we ask a question where the answer is indeed present in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "58b575af-8d60-44dc-9e7f-ffd0cb1a8204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ms. Agatha found an ancient wooden chest in her attic.\n"
     ]
    }
   ],
   "source": [
    "# Run the chain\n",
    "output = llm_chain.run(document=document, question=\"What did Ms. Agatha find in her attic?\", stop=[\"Question:\",\"\\n\"])\n",
    "\n",
    "print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc33936-87d7-418a-be07-bbc59d311212",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation\n",
    "---\n",
    "\n",
    "In the previous sections we saw a couple of things.\n",
    "\n",
    "- First, we did simple Q&A with the model\n",
    "- Second, we did some contextual QA with the model, where we gave it a piece of text (Document) and asked the model to answer questions from it.\n",
    "- Third, we went a bit further with the mechanism where we show some examples to the model as \"few shot\" and ask the question to the model.\n",
    "\n",
    "In the subsequent sections we will implement a RAG mechanism, step-by-step. RAG stands for \"Retriever-Augmented Generation\". It's a method in the domain of natural language processing (NLP) and information retrieval. RAG combines the powers of large pre-trained models like BERT (for information retrieval) and sequence-to-sequence models like BART or T5 (for generation) to produce answers to questions. Essentially, it retrieves relevant document passages from a corpus and then generates a response based on the information from those passages. To facilitate this, we will also take a look at vector databases, where we will store an entire document by first chunking it into smaller parts, and generating embeddings of those chunks, and finally loading them into the Vector DB. We will then see how we can do relevancy search on the Vecor DB to get text(s) relevant to our query, which will give us the basis of creating the context for the model. specifically, we will\n",
    "\n",
    "- Explore vector databases\n",
    "- Learn basics of QA exploring simple chains\n",
    "- Learn basics of chatbot\n",
    "- Build prompt templates for our chat bot\n",
    "- Explore various Chains useful for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff84015-923b-4e8f-b89a-4543f6755210",
   "metadata": {},
   "source": [
    "### Read the document\n",
    "\n",
    "Our final goal is to perform Q&A with the sample document `sagemaker-faqs.pdf`. First we need to read the text from the document and for that we will use Amazon Textract's LangChain integration. Since this is a multi-page (5 pages) document we will first have to upload it to Amazon S3 and then use that with `AmazonTextractPDFLoader` to read the document text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "c23022a8-a922-4f36-8077-234d8e4c9df4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker-us-east-1-965425568475\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "data_bucket = sagemaker.Session().default_bucket()\n",
    "print(data_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "11e36a8e-d621-40c2-9ab8-9f31503b1de8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "upload: ./sagemaker-faqs.pdf to s3://sagemaker-us-east-1-965425568475/rag/sagemaker-faqs.pdf\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./sagemaker-faqs.pdf s3://{data_bucket}/rag/sagemaker-faqs.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ca499b-1d7a-436d-82bc-4b0f4987a693",
   "metadata": {},
   "source": [
    "Let's load the document with AmazonTextractPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "43e20d19-27ce-4971-986d-f5c658ef816c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AmazonTextractPDFLoader\n",
    "\n",
    "loader = AmazonTextractPDFLoader(f\"s3://{data_bucket}/rag/sagemaker-faqs.pdf\")\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbe6484-b969-46e5-9539-0ae4dd54102d",
   "metadata": {},
   "source": [
    "Let's look at the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "f39bc024-b7d1-4728-b9ee-c64a009ee578",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========Page 1==========\n",
      "Amazon SageMaker SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows. AWS Regions For a list of the supported SageMaker Regions, please visit the AWS Regional Services page. Also, for more information, see Regional endpoints in the AWS general reference guide. Service availability of SageMaker SageMaker is designed for high availability. There are no maintenance windows or scheduled downtimes. SageMaker APIs run in Amazon proven high-availability data centers, with service stack replication configured across three facilities in each Region to provide fault tolerance in the event of a server failure or Availability Zone outage. Code Security SageMaker stores code in ML storage volumes, secured by security groups and optionally encrypted at rest. Security measures SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (AWS KMS) key to SageMaker notebooks, training jobs, and endpoints to encrypt the attached ML storage volume. SageMaker also supports Amazon Virtual Private Cloud (Amazon VPC) and AWS PrivateLink support. Sharing models, training data, or algorithms SageMaker does not use or share customer models, training data, or algorithms. We know that customers care deeply about privacy and data security. That's why AWS gives you ownership and control over your content through simplified, powerful tools that allow you to determine where your content will be stored, secure your content in transit and at rest, and manage your access to AWS services and resources for your users. We also implement technical and physical controls that are designed to prevent unauthorized access Amazon SageMaker SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows. AWS Regions For a list of the supported SageMaker Regions, please visit the AWS Regional Services page. Also, for more information, see Regional endpoints in the AWS general reference guide. Service availability of SageMaker SageMaker is designed for high availability. There are no maintenance windows or scheduled downtimes. SageMaker APIs run in Amazon proven high-availability data centers, with service stack replication configured across three facilities in each Region to provide fault tolerance in the event of a server failure or Availability Zone outage. Code Security SageMaker stores code in ML storage volumes, secured by security groups and optionally encrypted at rest. Security measures SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (AWS KMS) key to SageMaker notebooks, training jobs, and endpoints to encrypt the attached ML storage volume. SageMaker also supports Amazon Virtual Private Cloud (Amazon VPC) and AWS PrivateLink support. Sharing models, training data, or algorithms SageMaker does not use or share customer models, training data, or algorithms. We know that customers care deeply about privacy and data security. That's why AWS gives you ownership and control over your content through simplified, powerful tools that allow you to determine where your content will be stored, secure your content in transit and at rest, and manage your access to AWS services and resources for your users. We also implement technical and physical controls that are designed to prevent unauthorized access \n",
      "\n",
      "\n",
      "=========Page 2==========\n",
      "to or disclosure of your content. As a customer, you maintain ownership of your content, and you select which AWS services can process, store, and host your content. We do not access your content for any purpose without your consent. SageMaker Charges You pay for ML compute, storage, and data processing resources that you use for hosting the notebook, training the model, performing predictions, and logging the outputs. With SageMaker, you can select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. For more details, see Amazon SageMaker Pricing and the Amazon SageMaker Pricing Calculator. Cost Optimizations There are several best practices that you can adopt to optimize your SageMaker resource usage. Some approaches involve configuration optimizations; others involve programmatic solutions. A full guide on this concept, complete with visual tutorials and code samples, can be found in this blog post. Development environment SageMaker provides a full and complete workflow, but you can continue using your existing tools with SageMaker. You can easily transfer the results of each stage in and out of SageMaker as your business requirements dictate. Language support for R You can use R within SageMaker notebook instances, which include a preinstalled R kernel and the reticulate library. Reticulate offers an R interface for the Amazon SageMaker Python SDK, helping ML practitioners build, train, tune, and deploy R models. Model imbalances Amazon SageMaker Clarify helps improve model transparency by detecting statistical bias across the entire ML workflow. SageMaker Clarify checks for imbalances during data preparation, after training, and ongoing over time, and also includes tools to help explain ML models and their predictions. Findings can be shared through explainability reports. Bias detection with SageMaker Clarify Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different to or disclosure of your content. As a customer, you maintain ownership of your content, and you select which AWS services can process, store, and host your content. We do not access your content for any purpose without your consent. SageMaker Charges You pay for ML compute, storage, and data processing resources that you use for hosting the notebook, training the model, performing predictions, and logging the outputs. With SageMaker, you can select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. For more details, see Amazon SageMaker Pricing and the Amazon SageMaker Pricing Calculator. Cost Optimizations There are several best practices that you can adopt to optimize your SageMaker resource usage. Some approaches involve configuration optimizations; others involve programmatic solutions. A full guide on this concept, complete with visual tutorials and code samples, can be found in this blog post. Development environment SageMaker provides a full and complete workflow, but you can continue using your existing tools with SageMaker. You can easily transfer the results of each stage in and out of SageMaker as your business requirements dictate. Language support for R You can use R within SageMaker notebook instances, which include a preinstalled R kernel and the reticulate library. Reticulate offers an R interface for the Amazon SageMaker Python SDK, helping ML practitioners build, train, tune, and deploy R models. Model imbalances Amazon SageMaker Clarify helps improve model transparency by detecting statistical bias across the entire ML workflow. SageMaker Clarify checks for imbalances during data preparation, after training, and ongoing over time, and also includes tools to help explain ML models and their predictions. Findings can be shared through explainability reports. Bias detection with SageMaker Clarify Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different \n",
      "\n",
      "\n",
      "=========Page 3==========\n",
      "notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example). Model explainability with SageMaker Clarify SageMaker Clarify is integrated with SageMaker Experiments to provide a feature importance graph detailing the importance of each input for your model's overall decision-making process after the model has been trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify also makes explanations for individual predictions available through an API. Amazon SageMaker Studio SageMaker Studio provides a single, web-based visual interface where you can perform all ML development steps. SageMaker Studio gives you complete access, control, and visibility into each step required to prepare data and build, train, and deploy models. You can quickly upload data, create new notebooks, train and tune models, move back and forth between steps to adjust experiments, compare results, and deploy models to production all in one place, making you much more productive. All ML development activities including notebooks, experiment management, automatic model creation, debugging and profiling, and model drift detection can be performed within the unified SageMaker Studio visual interface. RStudio on Amazon SageMaker RStudio on SageMaker is the first fully managed RStudio Workbench in the cloud. You can quickly launch the familiar RStudio integrated development environment (IDE) and dial up and down the underlying compute resources without interrupting your work, making it easier to build ML and analytics solutions in R at scale. You can seamlessly switch between the RStudio IDE and SageMaker Studio notebooks for R and Python development. All your work, including code, datasets, repositories, and other artifacts, is notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example). Model explainability with SageMaker Clarify SageMaker Clarify is integrated with SageMaker Experiments to provide a feature importance graph detailing the importance of each input for your model's overall decision-making process after the model has been trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify also makes explanations for individual predictions available through an API. Amazon SageMaker Studio SageMaker Studio provides a single, web-based visual interface where you can perform all ML development steps. SageMaker Studio gives you complete access, control, and visibility into each step required to prepare data and build, train, and deploy models. You can quickly upload data, create new notebooks, train and tune models, move back and forth between steps to adjust experiments, compare results, and deploy models to production all in one place, making you much more productive. All ML development activities including notebooks, experiment management, automatic model creation, debugging and profiling, and model drift detection can be performed within the unified SageMaker Studio visual interface. RStudio on Amazon SageMaker RStudio on SageMaker is the first fully managed RStudio Workbench in the cloud. You can quickly launch the familiar RStudio integrated development environment (IDE) and dial up and down the underlying compute resources without interrupting your work, making it easier to build ML and analytics solutions in R at scale. You can seamlessly switch between the RStudio IDE and SageMaker Studio notebooks for R and Python development. All your work, including code, datasets, repositories, and other artifacts, is \n",
      "\n",
      "\n",
      "=========Page 4==========\n",
      "automatically synchronized between the two environments to reduce context switch and boost productivity. SageMaker Studio pricing There is no additional charge for using SageMaker Studio. You pay only for the underlying compute and storage charges on the services that you use within SageMaker Studio. Regions support for SageMaker Studio You can find the Regions where SageMaker Studio is supported in the Amazon SageMaker Developer Guide. ML governance ML governance tools in SageMaker SageMaker provides purpose-built ML governance tools across the ML lifecycle. With Amazon SageMaker Role Manager, administrators can define minimum permissions in minutes. Amazon SageMaker Model Cards makes it easier to capture, retrieve, and share essential model information from conception to deployment, and Amazon SageMaker Model Dashboard keeps you informed on production model behavior, all in one place. For more information, see ML Governance with Amazon SageMaker. SageMaker Role Manager You can define minimum permissions in minutes with SageMaker Role Manager. It provides a baseline set of permissions for ML activities and personas with a catalog of pre-built IAM policies. You can keep the baseline permissions, or customize them further based on your specific needs. With a few self-guided prompts, you can quickly input common governance constructs such as network access boundaries and encryption keys. SageMaker Role Manager will then generate the IAM policy automatically. You can discover the generated role and associated policies through the AWS IAM console. To further tailor the permissions to your use case, attach your managed IAM policies to the IAM role that you create with SageMaker Role Manager. You can also add tags to help identify the role and organize across AWS services. SageMaker Model Cards SageMaker Model Cards helps you centralize and standardize model documentation throughout the ML lifecycle by creating a single source of truth for model information. SageMaker Model Cards auto-populates training details to accelerate the documentation process. You can also add details such as the purpose of the model and the performance goals. You can attach model evaluation results to your model card and automatically synchronized between the two environments to reduce context switch and boost productivity. SageMaker Studio pricing There is no additional charge for using SageMaker Studio. You pay only for the underlying compute and storage charges on the services that you use within SageMaker Studio. Regions support for SageMaker Studio You can find the Regions where SageMaker Studio is supported in the Amazon SageMaker Developer Guide. ML governance ML governance tools in SageMaker SageMaker provides purpose-built ML governance tools across the ML lifecycle. With Amazon SageMaker Role Manager, administrators can define minimum permissions in minutes. Amazon SageMaker Model Cards makes it easier to capture, retrieve, and share essential model information from conception to deployment, and Amazon SageMaker Model Dashboard keeps you informed on production model behavior, all in one place. For more information, see ML Governance with Amazon SageMaker. SageMaker Role Manager You can define minimum permissions in minutes with SageMaker Role Manager. It provides a baseline set of permissions for ML activities and personas with a catalog of pre-built IAM policies. You can keep the baseline permissions, or customize them further based on your specific needs. With a few self-guided prompts, you can quickly input common governance constructs such as network access boundaries and encryption keys. SageMaker Role Manager will then generate the IAM policy automatically. You can discover the generated role and associated policies through the AWS IAM console. To further tailor the permissions to your use case, attach your managed IAM policies to the IAM role that you create with SageMaker Role Manager. You can also add tags to help identify the role and organize across AWS services. SageMaker Model Cards SageMaker Model Cards helps you centralize and standardize model documentation throughout the ML lifecycle by creating a single source of truth for model information. SageMaker Model Cards auto-populates training details to accelerate the documentation process. You can also add details such as the purpose of the model and the performance goals. You can attach model evaluation results to your model card and \n",
      "\n",
      "\n",
      "=========Page 5==========\n",
      "provide visualizations to gain key insights into model performance. SageMaker Model Cards can easily be shared with others by exporting to a PDF format. SageMaker Model Dashboard SageMaker Model Dashboard gives you a comprehensive overview of deployed models and endpoints, letting you track resources and model behavior violations through one pane. It allows you to monitor model behavior in four dimensions, including data and model quality, and bias and feature attribution drift through its integration with SageMaker Model Monitor and SageMaker Clarify. SageMaker Model Dashboard also provides an integrated experience to set up and receive alerts for missing and inactive model monitoring jobs, and deviations in model behavior for model quality, data quality, bias drift, and feature attribution drift. You can further inspect individual models and analyze factors impacting model performance over time. Then, you can follow up with ML practitioners to take corrective measures. Foundation models Getting started SageMaker JumpStart helps you quickly and easily get started with ML. SageMaker JumpStart provides a set of solutions for the most common use cases that can be deployed readily in just a few steps. The solutions are fully customizable and showcase the use of AWS CloudFormation templates and reference architectures so you can accelerate your ML journey. SageMaker JumpStart also provides foundation models and supports one-step deployment and fine-tuning of more than 150 popular open-source models, such as transformer, object detection, and image classification models. provide visualizations to gain key insights into model performance. SageMaker Model Cards can easily be shared with others by exporting to a PDF format. SageMaker Model Dashboard SageMaker Model Dashboard gives you a comprehensive overview of deployed models and endpoints, letting you track resources and model behavior violations through one pane. It allows you to monitor model behavior in four dimensions, including data and model quality, and bias and feature attribution drift through its integration with SageMaker Model Monitor and SageMaker Clarify. SageMaker Model Dashboard also provides an integrated experience to set up and receive alerts for missing and inactive model monitoring jobs, and deviations in model behavior for model quality, data quality, bias drift, and feature attribution drift. You can further inspect individual models and analyze factors impacting model performance over time. Then, you can follow up with ML practitioners to take corrective measures. Foundation models Getting started SageMaker JumpStart helps you quickly and easily get started with ML. SageMaker JumpStart provides a set of solutions for the most common use cases that can be deployed readily in just a few steps. The solutions are fully customizable and showcase the use of AWS CloudFormation templates and reference architectures so you can accelerate your ML journey. SageMaker JumpStart also provides foundation models and supports one-step deployment and fine-tuning of more than 150 popular open-source models, such as transformer, object detection, and image classification models. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index,page in enumerate(document):\n",
    "    print(f\"=========Page {index+1}==========\")\n",
    "    print(page.page_content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a9f6f-934b-4213-b6a0-ffd2c2ecd18d",
   "metadata": {},
   "source": [
    "As we can see, each page text has been extracted, but the total page text is likely too large for the model which only has 4k token context window. For this purpose we will split these pages further down into smaller chunks. We will do this using splitters available within LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "3b3537e6-653c-4ad7-ac7a-179e8f71fd2c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Chunk 1, From Page 1 ====\n",
      "Amazon SageMaker SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows. AWS Regions For a list of the supported SageMaker Regions, please visit the AWS Regional Services page. Also, for more information, see Regional endpoints in the AWS general reference guide\n",
      "\n",
      "\n",
      "==== Chunk 2, From Page 1 ====\n",
      ". Service availability of SageMaker SageMaker is designed for high availability. There are no maintenance windows or scheduled downtimes. SageMaker APIs run in Amazon proven high-availability data centers, with service stack replication configured across three facilities in each Region to provide fault tolerance in the event of a server failure or Availability Zone outage\n",
      "\n",
      "\n",
      "==== Chunk 3, From Page 1 ====\n",
      ". Code Security SageMaker stores code in ML storage volumes, secured by security groups and optionally encrypted at rest. Security measures SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection\n",
      "\n",
      "\n",
      "==== Chunk 4, From Page 1 ====\n",
      ". You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment\n",
      "\n",
      "\n",
      "==== Chunk 5, From Page 1 ====\n",
      ". You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (AWS KMS) key to SageMaker notebooks, training jobs, and endpoints to encrypt the attached ML storage volume. SageMaker also supports Amazon Virtual Private Cloud (Amazon VPC) and AWS PrivateLink support\n",
      "\n",
      "\n",
      "==== Chunk 6, From Page 1 ====\n",
      ". Sharing models, training data, or algorithms SageMaker does not use or share customer models, training data, or algorithms. We know that customers care deeply about privacy and data security\n",
      "\n",
      "\n",
      "==== Chunk 7, From Page 1 ====\n",
      ". That's why AWS gives you ownership and control over your content through simplified, powerful tools that allow you to determine where your content will be stored, secure your content in transit and at rest, and manage your access to AWS services and resources for your users\n",
      "\n",
      "\n",
      "==== Chunk 8, From Page 1 ====\n",
      ". We also implement technical and physical controls that are designed to prevent unauthorized access Amazon SageMaker SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows\n",
      "\n",
      "\n",
      "==== Chunk 9, From Page 1 ====\n",
      ". AWS Regions For a list of the supported SageMaker Regions, please visit the AWS Regional Services page. Also, for more information, see Regional endpoints in the AWS general reference guide. Service availability of SageMaker SageMaker is designed for high availability. There are no maintenance windows or scheduled downtimes\n",
      "\n",
      "\n",
      "==== Chunk 10, From Page 1 ====\n",
      ". SageMaker APIs run in Amazon proven high-availability data centers, with service stack replication configured across three facilities in each Region to provide fault tolerance in the event of a server failure or Availability Zone outage. Code Security SageMaker stores code in ML storage volumes, secured by security groups and optionally encrypted at rest\n",
      "\n",
      "\n",
      "==== Chunk 11, From Page 1 ====\n",
      ". Security measures SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment\n",
      "\n",
      "\n",
      "==== Chunk 12, From Page 1 ====\n",
      ". You can use encrypted Amazon Simple Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key Management Service (AWS KMS) key to SageMaker notebooks, training jobs, and endpoints to encrypt the attached ML storage volume. SageMaker also supports Amazon Virtual Private Cloud (Amazon VPC) and AWS PrivateLink support\n",
      "\n",
      "\n",
      "==== Chunk 13, From Page 1 ====\n",
      ". Sharing models, training data, or algorithms SageMaker does not use or share customer models, training data, or algorithms. We know that customers care deeply about privacy and data security\n",
      "\n",
      "\n",
      "==== Chunk 14, From Page 1 ====\n",
      ". That's why AWS gives you ownership and control over your content through simplified, powerful tools that allow you to determine where your content will be stored, secure your content in transit and at rest, and manage your access to AWS services and resources for your users. We also implement technical and physical controls that are designed to prevent unauthorized access\n",
      "\n",
      "\n",
      "==== Chunk 15, From Page 2 ====\n",
      "to or disclosure of your content. As a customer, you maintain ownership of your content, and you select which AWS services can process, store, and host your content. We do not access your content for any purpose without your consent\n",
      "\n",
      "\n",
      "==== Chunk 16, From Page 2 ====\n",
      ". SageMaker Charges You pay for ML compute, storage, and data processing resources that you use for hosting the notebook, training the model, performing predictions, and logging the outputs. With SageMaker, you can select the number and type of instance used for the hosted notebook, training, and model hosting\n",
      "\n",
      "\n",
      "==== Chunk 17, From Page 2 ====\n",
      ". You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. For more details, see Amazon SageMaker Pricing and the Amazon SageMaker Pricing Calculator. Cost Optimizations There are several best practices that you can adopt to optimize your SageMaker resource usage. Some approaches involve configuration optimizations; others involve programmatic solutions\n",
      "\n",
      "\n",
      "==== Chunk 18, From Page 2 ====\n",
      ". A full guide on this concept, complete with visual tutorials and code samples, can be found in this blog post. Development environment SageMaker provides a full and complete workflow, but you can continue using your existing tools with SageMaker. You can easily transfer the results of each stage in and out of SageMaker as your business requirements dictate\n",
      "\n",
      "\n",
      "==== Chunk 19, From Page 2 ====\n",
      ". Language support for R You can use R within SageMaker notebook instances, which include a preinstalled R kernel and the reticulate library. Reticulate offers an R interface for the Amazon SageMaker Python SDK, helping ML practitioners build, train, tune, and deploy R models\n",
      "\n",
      "\n",
      "==== Chunk 20, From Page 2 ====\n",
      ". Model imbalances Amazon SageMaker Clarify helps improve model transparency by detecting statistical bias across the entire ML workflow. SageMaker Clarify checks for imbalances during data preparation, after training, and ongoing over time, and also includes tools to help explain ML models and their predictions. Findings can be shared through explainability reports\n",
      "\n",
      "\n",
      "==== Chunk 21, From Page 2 ====\n",
      ". Bias detection with SageMaker Clarify Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different to or disclosure of your content\n",
      "\n",
      "\n",
      "==== Chunk 22, From Page 2 ====\n",
      ". As a customer, you maintain ownership of your content, and you select which AWS services can process, store, and host your content. We do not access your content for any purpose without your consent. SageMaker Charges You pay for ML compute, storage, and data processing resources that you use for hosting the notebook, training the model, performing predictions, and logging the outputs\n",
      "\n",
      "\n",
      "==== Chunk 23, From Page 2 ====\n",
      ". With SageMaker, you can select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. For more details, see Amazon SageMaker Pricing and the Amazon SageMaker Pricing Calculator\n",
      "\n",
      "\n",
      "==== Chunk 24, From Page 2 ====\n",
      ". Cost Optimizations There are several best practices that you can adopt to optimize your SageMaker resource usage. Some approaches involve configuration optimizations; others involve programmatic solutions. A full guide on this concept, complete with visual tutorials and code samples, can be found in this blog post\n",
      "\n",
      "\n",
      "==== Chunk 25, From Page 2 ====\n",
      ". Development environment SageMaker provides a full and complete workflow, but you can continue using your existing tools with SageMaker. You can easily transfer the results of each stage in and out of SageMaker as your business requirements dictate. Language support for R You can use R within SageMaker notebook instances, which include a preinstalled R kernel and the reticulate library\n",
      "\n",
      "\n",
      "==== Chunk 26, From Page 2 ====\n",
      ". Reticulate offers an R interface for the Amazon SageMaker Python SDK, helping ML practitioners build, train, tune, and deploy R models. Model imbalances Amazon SageMaker Clarify helps improve model transparency by detecting statistical bias across the entire ML workflow\n",
      "\n",
      "\n",
      "==== Chunk 27, From Page 2 ====\n",
      ". SageMaker Clarify checks for imbalances during data preparation, after training, and ongoing over time, and also includes tools to help explain ML models and their predictions. Findings can be shared through explainability reports. Bias detection with SageMaker Clarify Measuring bias in ML models is a first step to mitigating bias\n",
      "\n",
      "\n",
      "==== Chunk 28, From Page 2 ====\n",
      ". Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different\n",
      "\n",
      "\n",
      "==== Chunk 29, From Page 3 ====\n",
      "notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation\n",
      "\n",
      "\n",
      "==== Chunk 30, From Page 3 ====\n",
      ". SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor)\n",
      "\n",
      "\n",
      "==== Chunk 31, From Page 3 ====\n",
      ". For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups\n",
      "\n",
      "\n",
      "==== Chunk 32, From Page 3 ====\n",
      ". For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example)\n",
      "\n",
      "\n",
      "==== Chunk 33, From Page 3 ====\n",
      ". Model explainability with SageMaker Clarify SageMaker Clarify is integrated with SageMaker Experiments to provide a feature importance graph detailing the importance of each input for your model's overall decision-making process after the model has been trained. These details can help determine if a particular model input has more influence than it should on overall model behavior\n",
      "\n",
      "\n",
      "==== Chunk 34, From Page 3 ====\n",
      ". SageMaker Clarify also makes explanations for individual predictions available through an API. Amazon SageMaker Studio SageMaker Studio provides a single, web-based visual interface where you can perform all ML development steps. SageMaker Studio gives you complete access, control, and visibility into each step required to prepare data and build, train, and deploy models\n",
      "\n",
      "\n",
      "==== Chunk 35, From Page 3 ====\n",
      ". You can quickly upload data, create new notebooks, train and tune models, move back and forth between steps to adjust experiments, compare results, and deploy models to production all in one place, making you much more productive\n",
      "\n",
      "\n",
      "==== Chunk 36, From Page 3 ====\n",
      ". All ML development activities including notebooks, experiment management, automatic model creation, debugging and profiling, and model drift detection can be performed within the unified SageMaker Studio visual interface. RStudio on Amazon SageMaker RStudio on SageMaker is the first fully managed RStudio Workbench in the cloud\n",
      "\n",
      "\n",
      "==== Chunk 37, From Page 3 ====\n",
      ". You can quickly launch the familiar RStudio integrated development environment (IDE) and dial up and down the underlying compute resources without interrupting your work, making it easier to build ML and analytics solutions in R at scale. You can seamlessly switch between the RStudio IDE and SageMaker Studio notebooks for R and Python development\n",
      "\n",
      "\n",
      "==== Chunk 38, From Page 3 ====\n",
      ". All your work, including code, datasets, repositories, and other artifacts, is notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation\n",
      "\n",
      "\n",
      "==== Chunk 39, From Page 3 ====\n",
      ". SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor)\n",
      "\n",
      "\n",
      "==== Chunk 40, From Page 3 ====\n",
      ". For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups\n",
      "\n",
      "\n",
      "==== Chunk 41, From Page 3 ====\n",
      ". For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example)\n",
      "\n",
      "\n",
      "==== Chunk 42, From Page 3 ====\n",
      ". Model explainability with SageMaker Clarify SageMaker Clarify is integrated with SageMaker Experiments to provide a feature importance graph detailing the importance of each input for your model's overall decision-making process after the model has been trained. These details can help determine if a particular model input has more influence than it should on overall model behavior\n",
      "\n",
      "\n",
      "==== Chunk 43, From Page 3 ====\n",
      ". SageMaker Clarify also makes explanations for individual predictions available through an API. Amazon SageMaker Studio SageMaker Studio provides a single, web-based visual interface where you can perform all ML development steps. SageMaker Studio gives you complete access, control, and visibility into each step required to prepare data and build, train, and deploy models\n",
      "\n",
      "\n",
      "==== Chunk 44, From Page 3 ====\n",
      ". You can quickly upload data, create new notebooks, train and tune models, move back and forth between steps to adjust experiments, compare results, and deploy models to production all in one place, making you much more productive\n",
      "\n",
      "\n",
      "==== Chunk 45, From Page 3 ====\n",
      ". All ML development activities including notebooks, experiment management, automatic model creation, debugging and profiling, and model drift detection can be performed within the unified SageMaker Studio visual interface. RStudio on Amazon SageMaker RStudio on SageMaker is the first fully managed RStudio Workbench in the cloud\n",
      "\n",
      "\n",
      "==== Chunk 46, From Page 3 ====\n",
      ". You can quickly launch the familiar RStudio integrated development environment (IDE) and dial up and down the underlying compute resources without interrupting your work, making it easier to build ML and analytics solutions in R at scale. You can seamlessly switch between the RStudio IDE and SageMaker Studio notebooks for R and Python development\n",
      "\n",
      "\n",
      "==== Chunk 47, From Page 3 ====\n",
      ". All your work, including code, datasets, repositories, and other artifacts, is\n",
      "\n",
      "\n",
      "==== Chunk 48, From Page 4 ====\n",
      "automatically synchronized between the two environments to reduce context switch and boost productivity. SageMaker Studio pricing There is no additional charge for using SageMaker Studio. You pay only for the underlying compute and storage charges on the services that you use within SageMaker Studio\n",
      "\n",
      "\n",
      "==== Chunk 49, From Page 4 ====\n",
      ". Regions support for SageMaker Studio You can find the Regions where SageMaker Studio is supported in the Amazon SageMaker Developer Guide. ML governance ML governance tools in SageMaker SageMaker provides purpose-built ML governance tools across the ML lifecycle. With Amazon SageMaker Role Manager, administrators can define minimum permissions in minutes\n",
      "\n",
      "\n",
      "==== Chunk 50, From Page 4 ====\n",
      ". Amazon SageMaker Model Cards makes it easier to capture, retrieve, and share essential model information from conception to deployment, and Amazon SageMaker Model Dashboard keeps you informed on production model behavior, all in one place. For more information, see ML Governance with Amazon SageMaker\n",
      "\n",
      "\n",
      "==== Chunk 51, From Page 4 ====\n",
      ". SageMaker Role Manager You can define minimum permissions in minutes with SageMaker Role Manager. It provides a baseline set of permissions for ML activities and personas with a catalog of pre-built IAM policies. You can keep the baseline permissions, or customize them further based on your specific needs\n",
      "\n",
      "\n",
      "==== Chunk 52, From Page 4 ====\n",
      ". With a few self-guided prompts, you can quickly input common governance constructs such as network access boundaries and encryption keys. SageMaker Role Manager will then generate the IAM policy automatically. You can discover the generated role and associated policies through the AWS IAM console\n",
      "\n",
      "\n",
      "==== Chunk 53, From Page 4 ====\n",
      ". To further tailor the permissions to your use case, attach your managed IAM policies to the IAM role that you create with SageMaker Role Manager. You can also add tags to help identify the role and organize across AWS services\n",
      "\n",
      "\n",
      "==== Chunk 54, From Page 4 ====\n",
      ". SageMaker Model Cards SageMaker Model Cards helps you centralize and standardize model documentation throughout the ML lifecycle by creating a single source of truth for model information. SageMaker Model Cards auto-populates training details to accelerate the documentation process. You can also add details such as the purpose of the model and the performance goals\n",
      "\n",
      "\n",
      "==== Chunk 55, From Page 4 ====\n",
      ". You can attach model evaluation results to your model card and automatically synchronized between the two environments to reduce context switch and boost productivity. SageMaker Studio pricing There is no additional charge for using SageMaker Studio. You pay only for the underlying compute and storage charges on the services that you use within SageMaker Studio\n",
      "\n",
      "\n",
      "==== Chunk 56, From Page 4 ====\n",
      ". Regions support for SageMaker Studio You can find the Regions where SageMaker Studio is supported in the Amazon SageMaker Developer Guide. ML governance ML governance tools in SageMaker SageMaker provides purpose-built ML governance tools across the ML lifecycle. With Amazon SageMaker Role Manager, administrators can define minimum permissions in minutes\n",
      "\n",
      "\n",
      "==== Chunk 57, From Page 4 ====\n",
      ". Amazon SageMaker Model Cards makes it easier to capture, retrieve, and share essential model information from conception to deployment, and Amazon SageMaker Model Dashboard keeps you informed on production model behavior, all in one place. For more information, see ML Governance with Amazon SageMaker\n",
      "\n",
      "\n",
      "==== Chunk 58, From Page 4 ====\n",
      ". SageMaker Role Manager You can define minimum permissions in minutes with SageMaker Role Manager. It provides a baseline set of permissions for ML activities and personas with a catalog of pre-built IAM policies. You can keep the baseline permissions, or customize them further based on your specific needs\n",
      "\n",
      "\n",
      "==== Chunk 59, From Page 4 ====\n",
      ". With a few self-guided prompts, you can quickly input common governance constructs such as network access boundaries and encryption keys. SageMaker Role Manager will then generate the IAM policy automatically. You can discover the generated role and associated policies through the AWS IAM console\n",
      "\n",
      "\n",
      "==== Chunk 60, From Page 4 ====\n",
      ". To further tailor the permissions to your use case, attach your managed IAM policies to the IAM role that you create with SageMaker Role Manager. You can also add tags to help identify the role and organize across AWS services\n",
      "\n",
      "\n",
      "==== Chunk 61, From Page 4 ====\n",
      ". SageMaker Model Cards SageMaker Model Cards helps you centralize and standardize model documentation throughout the ML lifecycle by creating a single source of truth for model information. SageMaker Model Cards auto-populates training details to accelerate the documentation process. You can also add details such as the purpose of the model and the performance goals\n",
      "\n",
      "\n",
      "==== Chunk 62, From Page 4 ====\n",
      ". You can attach model evaluation results to your model card and\n",
      "\n",
      "\n",
      "==== Chunk 63, From Page 5 ====\n",
      "provide visualizations to gain key insights into model performance. SageMaker Model Cards can easily be shared with others by exporting to a PDF format. SageMaker Model Dashboard SageMaker Model Dashboard gives you a comprehensive overview of deployed models and endpoints, letting you track resources and model behavior violations through one pane\n",
      "\n",
      "\n",
      "==== Chunk 64, From Page 5 ====\n",
      ". It allows you to monitor model behavior in four dimensions, including data and model quality, and bias and feature attribution drift through its integration with SageMaker Model Monitor and SageMaker Clarify\n",
      "\n",
      "\n",
      "==== Chunk 65, From Page 5 ====\n",
      ". SageMaker Model Dashboard also provides an integrated experience to set up and receive alerts for missing and inactive model monitoring jobs, and deviations in model behavior for model quality, data quality, bias drift, and feature attribution drift. You can further inspect individual models and analyze factors impacting model performance over time\n",
      "\n",
      "\n",
      "==== Chunk 66, From Page 5 ====\n",
      ". Then, you can follow up with ML practitioners to take corrective measures. Foundation models Getting started SageMaker JumpStart helps you quickly and easily get started with ML. SageMaker JumpStart provides a set of solutions for the most common use cases that can be deployed readily in just a few steps\n",
      "\n",
      "\n",
      "==== Chunk 67, From Page 5 ====\n",
      ". The solutions are fully customizable and showcase the use of AWS CloudFormation templates and reference architectures so you can accelerate your ML journey. SageMaker JumpStart also provides foundation models and supports one-step deployment and fine-tuning of more than 150 popular open-source models, such as transformer, object detection, and image classification models\n",
      "\n",
      "\n",
      "==== Chunk 68, From Page 5 ====\n",
      ". provide visualizations to gain key insights into model performance. SageMaker Model Cards can easily be shared with others by exporting to a PDF format. SageMaker Model Dashboard SageMaker Model Dashboard gives you a comprehensive overview of deployed models and endpoints, letting you track resources and model behavior violations through one pane\n",
      "\n",
      "\n",
      "==== Chunk 69, From Page 5 ====\n",
      ". It allows you to monitor model behavior in four dimensions, including data and model quality, and bias and feature attribution drift through its integration with SageMaker Model Monitor and SageMaker Clarify\n",
      "\n",
      "\n",
      "==== Chunk 70, From Page 5 ====\n",
      ". SageMaker Model Dashboard also provides an integrated experience to set up and receive alerts for missing and inactive model monitoring jobs, and deviations in model behavior for model quality, data quality, bias drift, and feature attribution drift. You can further inspect individual models and analyze factors impacting model performance over time\n",
      "\n",
      "\n",
      "==== Chunk 71, From Page 5 ====\n",
      ". Then, you can follow up with ML practitioners to take corrective measures. Foundation models Getting started SageMaker JumpStart helps you quickly and easily get started with ML. SageMaker JumpStart provides a set of solutions for the most common use cases that can be deployed readily in just a few steps\n",
      "\n",
      "\n",
      "==== Chunk 72, From Page 5 ====\n",
      ". The solutions are fully customizable and showcase the use of AWS CloudFormation templates and reference architectures so you can accelerate your ML journey. SageMaker JumpStart also provides foundation models and supports one-step deployment and fine-tuning of more than 150 popular open-source models, such as transformer, object detection, and image classification models.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400,\n",
    "                                               separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "                                               chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(document)\n",
    "\n",
    "for index, text in enumerate(texts):\n",
    "    print(f\"==== Chunk {index+1}, From Page {text.metadata['page']} ====\")\n",
    "    print(text.page_content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb9c76-aeb4-4b31-80c0-803bfed0c417",
   "metadata": {},
   "source": [
    "We have split the document into smaller chunks. We will now perform a couple of things-\n",
    "\n",
    "- Generate embeddings of these chunks\n",
    "- Store these embeddings into a vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad8c530-e385-4129-8912-1d2677c1b52c",
   "metadata": {},
   "source": [
    "### Vector store indexer\n",
    "\n",
    "This is what stores and matches the embeddings. This notebook showcases FAISS and will be transient and in memory. FAISS (Facebook AI Similarity Search) is a library that allows developers to quickly search for embeddings of multimedia documents that are similar to each other. It solves limitations of traditional query search engines that are optimized for hash-based searches, and provides more scalable similarity search functions. The VectorStore APIs that use FAISS within LangChain are available [here](https://python.langchain.com/en/harrison-docs-refactor-3-24/reference/modules/vectorstore.html). You can read up about FAISS in memory vector store [here](https://arxiv.org/pdf/1702.08734.pdf).\n",
    "\n",
    "Some other notable Vector databases are\n",
    "\n",
    "- [Chroma](https://www.trychroma.com/) is a super simple vector search database. The core-API consists of just four functions, allowing users to build an in-memory document-vector store. By default Chroma uses the Hugging Face transformers library to vectorize documents.\n",
    "- [Weaviate](https://github.com/weaviate/weaviate) is a very posh looking tool - not only does Weaviate offer a GraphQL API with support for vector search. It also allows users to vectorize their content using Weaviate's inbuilt modules or custom modules.\n",
    "\n",
    "We will use `HuggingFaceEmbeddings` available via LangChain to generate embeddings of our text chunks that we generated in the previous step. This will be used by the FAISS (or Chroma) to store in memory and be used when ever the User runs a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "b9ad00af-6a7a-4857-aaa0-ace2a6a943d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_db = FAISS.from_documents(documents=texts,\n",
    "                                                               embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "3fb63d31-2313-4329-bb74-333d8d93ba72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we are loading the FAISS Vector DB in memory, it will load into the SageMaker Studio instance's memory\n",
    "# you may want to free up memory from time to time. To do that, uncomment the line below and execute this cell\n",
    "\n",
    "# CAUTION! This will delete the vector index\n",
    "\n",
    "# vector_db.delete([vector_db.index_to_docstore_id[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d65de05-7353-4ca8-adea-5ef68835e1db",
   "metadata": {},
   "source": [
    "We have loaded our vector db with the document, now let's run a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "3c7f29a7-f04d-4840-9b3b-a2fb756d2bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"How am I charged for sagemaker?\"\n",
    "docs = vector_db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "a5995cc8-ab61-499f-8ed4-357544051104",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. For more details, see Amazon SageMaker Pricing and the Amazon SageMaker Pricing Calculator. Cost Optimizations There are several best practices that you can adopt to optimize your SageMaker resource usage. Some approaches involve configuration optimizations; others involve programmatic solutions', metadata={'source': 's3://sagemaker-us-east-1-965425568475/rag/sagemaker-faqs.pdf', 'page': 2}),\n",
       " Document(page_content='. SageMaker Charges You pay for ML compute, storage, and data processing resources that you use for hosting the notebook, training the model, performing predictions, and logging the outputs. With SageMaker, you can select the number and type of instance used for the hosted notebook, training, and model hosting', metadata={'source': 's3://sagemaker-us-east-1-965425568475/rag/sagemaker-faqs.pdf', 'page': 2}),\n",
       " Document(page_content='. As a customer, you maintain ownership of your content, and you select which AWS services can process, store, and host your content. We do not access your content for any purpose without your consent. SageMaker Charges You pay for ML compute, storage, and data processing resources that you use for hosting the notebook, training the model, performing predictions, and logging the outputs', metadata={'source': 's3://sagemaker-us-east-1-965425568475/rag/sagemaker-faqs.pdf', 'page': 2}),\n",
       " Document(page_content='. With SageMaker, you can select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. For more details, see Amazon SageMaker Pricing and the Amazon SageMaker Pricing Calculator', metadata={'source': 's3://sagemaker-us-east-1-965425568475/rag/sagemaker-faqs.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85230c-9805-4c0c-b7f2-f129626eece8",
   "metadata": {},
   "source": [
    "The query returns all the chunks from the document that is similar to the `query`, by default it returns the Top 3 similar chunks. Let's see how to return just Top 2 with confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "9ab0c6d8-d3c0-4cfa-9a58-4b79b8c266cf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. For more details, see Amazon SageMaker Pricing and the Amazon SageMaker Pricing Calculator. Cost Optimizations There are several best practices that you can adopt to optimize your SageMaker resource usage. Some approaches involve configuration optimizations; others involve programmatic solutions', metadata={'source': 's3://sagemaker-us-east-1-965425568475/rag/sagemaker-faqs.pdf', 'page': 2}),\n",
       "  0.4935473),\n",
       " (Document(page_content='. SageMaker Charges You pay for ML compute, storage, and data processing resources that you use for hosting the notebook, training the model, performing predictions, and logging the outputs. With SageMaker, you can select the number and type of instance used for the hosted notebook, training, and model hosting', metadata={'source': 's3://sagemaker-us-east-1-965425568475/rag/sagemaker-faqs.pdf', 'page': 2}),\n",
       "  0.55405664)]"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = vector_db.similarity_search_with_score(query, k = 2)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a53f54-eb46-41fe-a9e7-3696479f847d",
   "metadata": {},
   "source": [
    "### Vector store-backed retriever\n",
    "---\n",
    "\n",
    "According to LangChain documentation-\n",
    "\n",
    "> A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store.\n",
    "\n",
    "Wrapping our vector db in a retriever wrapper is going to be useful when we use it in the Q&A chain for our chatbot in subsequent sections. But let's take a look how it works. The functionality is pretty similar to before (i.e. querying) with a slightly different interface.\n",
    "\n",
    "We first define a retriever with search type `mmr` (Max Marginal Relevance),  other option is `similarity`.  Note that the `search_type` depends on which vector DB you are using, some vector DBs may or may not support `mmr` etc. \n",
    "\n",
    "> MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document.\n",
    "\n",
    "We also define how many top results to return, in this case 2. Finally we query the retriever using `get_relevant_documents` by passing in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "d6144f13-3aab-4b90-9420-06778daaeea5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='. Cost Optimizations There are several best practices that you can adopt to optimize your SageMaker resource usage. Some approaches involve configuration optimizations; others involve programmatic solutions. A full guide on this concept, complete with visual tutorials and code samples, can be found in this blog post', metadata={'source': 's3://sagemaker-us-east-1-965425568475/rag/sagemaker-faqs.pdf', 'page': 2}),\n",
       " Document(page_content='. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor)', metadata={'source': 's3://sagemaker-us-east-1-965425568475/rag/sagemaker-faqs.pdf', 'page': 3}),\n",
       " Document(page_content='. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. For more details, see Amazon SageMaker Pricing and the Amazon SageMaker Pricing Calculator. Cost Optimizations There are several best practices that you can adopt to optimize your SageMaker resource usage. Some approaches involve configuration optimizations; others involve programmatic solutions', metadata={'source': 's3://sagemaker-us-east-1-965425568475/rag/sagemaker-faqs.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How do I cost optimize sagemaker?\"\n",
    "\n",
    "retriever = vector_db.as_retriever(search_type='mmr', search_kwargs={\"k\": 3})\n",
    "relevant_docs = retriever.get_relevant_documents(query)   \n",
    "relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0138c64b-c16b-47b7-9ab6-5def72fba116",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build context from retrieved documents\n",
    "---\n",
    "\n",
    "We now have the two relevant pieces of text that \"contain\" the anwer to our question, we are not quite there yet. So we will use a technique that we used earlier to build context and ask the quetion to the Llama-2 model. In this case, we will use the two text chunks we retrieved from the vector db to create the context by simply concatenating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "39e71ca5-89a0-459b-ae2c-37c4c64e2eea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost Optimizations There are several best practices that you can adopt to optimize your SageMaker resource usage. Some approaches involve configuration optimizations; others involve programmatic solutions. A full guide on this concept, complete with visual tutorials and code samples, can be found in this blog post . SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor) . You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. For more details, see Amazon SageMaker Pricing and the Amazon SageMaker Pricing Calculator. Cost Optimizations There are several best practices that you can adopt to optimize your SageMaker resource usage. Some approaches involve configuration optimizations; others involve programmatic solutions\n"
     ]
    }
   ],
   "source": [
    "full_context = str()\n",
    "for doc in relevant_docs:\n",
    "    full_context += doc.page_content+\" \"\n",
    "    \n",
    "print(full_context.strip(\".\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "bb4e9560-7a2e-446a-abea-3c087a130d15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_llm=SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name, \n",
    "        region_name=aws_region,\n",
    "        model_kwargs={\"do_sample\": False,\n",
    "                                    \"top_p\": 0.9,\n",
    "                                    \"temperature\": 0.5,\n",
    "                                    \"max_new_tokens\":  200,\n",
    "                                    \"stop\": [\"<|endoftext|>\", \"</s>\"]},\n",
    "        content_handler=content_handler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "163e1248-279a-4190-a33d-8613ae3e04aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text does not provide a specific answer to the question \"How do I optimize SageMaker?\". However, it does mention several best practices that can be adopted to optimize resource usage, such as configuration optimizations and programmatic solutions. It also provides links to a blog post and Amazon SageMaker documentation for further guidance.\n"
     ]
    }
   ],
   "source": [
    "# template = \"\"\"Answer the question as truthfully as possible using the provided text. If the answer is not contained within the text below, say \"I don't know\", do not make up an answer.  \n",
    "# Text: {document}\n",
    "# Question: {question}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "template = \"\"\">>INTRODUCTION<<Answer the question as truthfully as possible strictly using only the provided text, and if the answer is not contained within the text, say \"I don't know\". Make sure your answer is verbatim from the provided text. \n",
    ">>SUMMARY<<{document}\n",
    ">>QUESTION<<{question}\n",
    ">>ANSWER<<\"\"\"\n",
    "\n",
    "\n",
    "# define the prompt template\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"document\",\"question\"])\n",
    "\n",
    "# define an LLMChain\n",
    "llm_chain = LLMChain(prompt=qa_prompt, llm=sm_llm)\n",
    "\n",
    "query = \"How do I optimize sagemaker?\"\n",
    "\n",
    "# Run the chain with document as full_context and question as query we defined earlier\n",
    "output = llm_chain.run(document=full_context, question=query)\n",
    "\n",
    "print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49717aa-d417-4e1f-84f2-23e39fd2244c",
   "metadata": {
    "tags": []
   },
   "source": [
    "That's a much better and concise answer. Let's try another question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "e316c21d-c243-4fa9-bd16-1e9466c6003c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To share models, you can use Amazon SageMaker's built-in sharing capabilities. You can create a model package from your trained model and then share it with others. You can also use SageMaker's model registry to share models across multiple accounts or teams. Additionally, you can use SageMaker's model deployment capabilities to deploy your models to production environments.\n"
     ]
    }
   ],
   "source": [
    "# define the prompt template\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"document\",\"question\"])\n",
    "\n",
    "# define an LLMChain\n",
    "llm_chain = LLMChain(prompt=qa_prompt, llm=sm_llm)\n",
    "\n",
    "query_1=\"How do I share models?\"\n",
    "output = llm_chain.run(document=full_context, question=query_1)\n",
    "\n",
    "print(output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885cf1cb-4f04-45da-a03e-4c8cce83c6fa",
   "metadata": {},
   "source": [
    "The model is unable to answer this specific question. That is because our `full_context` doesn't have any information related to the question. So we will have to again do a similarity search from the vector database to get the relevant chunks of text, then build the context with those chunks and then as the question to the LLM with that context. But that is a lot of repeated steps, and we can certainly write reusable functions to do it. However, there is a much easier way to achieve this using \"QA Chain\" available in LangChain, with just a few lines of code. So let's see how that works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bbcc7d-2109-474e-8f78-9bc09a967ee4",
   "metadata": {},
   "source": [
    "### Performing Q&A with RAG with `load_qa_chain`\n",
    "---\n",
    "\n",
    "For this purpose, we will first define a question, and then generate embeddings from it. Once we have that we can perform similarity search on the vector database to find relevant pieces of information from the document. These relevant pieces of information will then be passed on to the model so that it can answer the question. We will use LangChain's `load_qa_chain` to perform Q&A with the model. The load qa chain does the work with prompt creation and all the context generation with help from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "a8c20c53-1902-4c03-9c39-16217af77f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SageMaker Model Cards are a way to capture, retrieve, and share essential model information from conception to deployment. They provide a concise summary of the model's purpose, data sources, training details, and intended use cases. The cards also include information about the model's accuracy, bias, and limitations.\n"
     ]
    }
   ],
   "source": [
    "# from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "\n",
    "retriever = vector_db.as_retriever(search_type='mmr', search_kwargs={\"k\": 2})\n",
    "\n",
    "# template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "template = \"\"\">>INTRODUCTION<<Answer the question as truthfully as possible strictly using only the provided text, and if the answer is not contained within the text, say \"I don't know\". Make sure your answer is verbatim from the provided text. \n",
    ">>SUMMARY<<{context}\n",
    ">>QUESTION<<{question}\n",
    ">>ANSWER<<\"\"\"\n",
    "\n",
    "# define the prompt template\n",
    "qa_prompt = PromptTemplate(template=template, input_variables=[\"context\",\"question\"])\n",
    "\n",
    "chain_type_kwargs = { \"prompt\": qa_prompt }\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=sm_llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")\n",
    "\n",
    "question=\"What are SageMaker Model cards?\"\n",
    "\n",
    "result = qa.run(question)\n",
    "print(result.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5cdc0",
   "metadata": {},
   "source": [
    "## Chatbot application\n",
    "\n",
    "#### For the chatbot we need `context management, history, vector stores, and many other things`. We will start by with a ConversationalRetrievalChain\n",
    "\n",
    "This uses conversation memory and RetrievalQAChain which Allow for passing in chat history which can be used for follow up questions.Source: https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html\n",
    "\n",
    "Set verbose to True to see all the what is going on behind the scenes\n",
    "\n",
    "**We use Custom Prompt template to fine tune the output responses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "42367663",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat bot\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "['Enter your query, q to quit'] What is Sagemaker?\n",
      "['Enter your query, q to quit', 'Question:What is Sagemaker?\\nAI:Answer: Sagemaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.'] How does SageMaker help mitigate bias?\n",
      "['Enter your query, q to quit', 'Question:What is Sagemaker?\\nAI:Answer: Sagemaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.', 'Question:How does SageMaker help mitigate bias?\\nAI:Answer:\\nSageMaker takes several measures to mitigate bias in machine learning models. These measures include:\\n\\n1. Data Collection and Labeling: SageMaker provides tools and services to help customers collect and label data in a way that minimizes bias. For example, SageMaker Ground Truth provides a platform for labeling data and managing workflows.\\n\\n2. Model Selection and Training: SageMaker provides tools and services to help customers select and train models that are less likely to be biased. For example, SageMaker provides algorithms that can help detect and mitigate bias in training data.\\n\\n3. Model Evaluation and Monitoring: SageMaker provides tools and services to help customers evaluate and monitor their models for bias. For example, SageMaker provides algorithms that can help detect and mitigate bias in inference data.\\n\\n4. Transparency and Accountability: SageMaker provides tools and services to help customers understand how their models work and to identify potential sources of bias. For example, SageMaker provides algorithms that can help explain the'] quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking\n",
      "Thank you , that was a nice chat !!\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "\n",
    "def create_prompt_template():\n",
    "    _template = \"\"\"\n",
    "    Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you do not know, do not try to make up an answer.\n",
    "        Chat History:\n",
    "        {chat_history}\n",
    "        Follow Up Input: {question}\n",
    "        Standalone question:\n",
    "    \"\"\"\n",
    "    CONVO_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "    return CONVO_QUESTION_PROMPT\n",
    "memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\", return_messages=True)\n",
    "chat_history=[]\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=sm_llm, \n",
    "    #retriever=vectorstore_faiss_aws.as_retriever(), \n",
    "    retriever=retriever,\n",
    "    memory=memory_chain,\n",
    "    #verbose=True,\n",
    "    condense_question_prompt=create_prompt_template(), #CONDENSE_QUESTION_PROMPT, # use the condense prompt template\n",
    "    #chain_type='map_reduce',\n",
    "    max_tokens_limit=100\n",
    "    #combine_docs_chain_kwargs=key_chain_args,\n",
    "\n",
    ")\n",
    "print(\"Starting chat bot\")\n",
    "input_str = ['Enter your query, q to quit']\n",
    "while True:\n",
    "    query = input(str(input_str))\n",
    "    if 'q' == query or 'quit' == query or 'Q' == query:\n",
    "        print(\"Breaking\")\n",
    "        break\n",
    "    else:\n",
    "        result = qa.run({'question':query, 'chat_history':chat_history} )\n",
    "        input_str.append(f\"Question:{query}\\nAI:Answer:{result}\")\n",
    "\n",
    "print(\"Thank you , that was a nice chat !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffdaaf4-17e6-45c8-acca-7c9905b8c556",
   "metadata": {},
   "source": [
    "#### Refine as Chain type with no similiarity searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "257cf2f5-345b-4473-af92-ea29de832e01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Refine chat bot\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "['Enter your query, q to quit'] What is Amazon SageMaker? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomHFEmbeddings::QUERY::shape:returned -- > (768,):\n",
      "ContentHandlerSMLMI::LangChain:::LEN:input_str=203:: will truncate if > 10000::\n",
      "ContentHandlerSMLMI::LangChain::output={'generated_texts': ['                                                                                                   ']}:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "['Enter your query, q to quit', 'Question:What is Amazon SageMaker? \\nAI:Answer:                                                                                                   '] quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking\n",
      "Thank you , that was a nice chat !!\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "\n",
    "def create_prompt_template():\n",
    "    \n",
    "\n",
    "    _template = \"\"\"\n",
    "    Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you do not know, do not try to make up an answer.\n",
    "        Chat History:\n",
    "        {chat_history}\n",
    "        Follow Up Input: {question}\n",
    "        Standalone question:\n",
    "    \"\"\"\n",
    "    CONVO_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "    return CONVO_QUESTION_PROMPT\n",
    "memory_chain = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\", return_messages=True)\n",
    "chat_history=[]\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=sm_llm, \n",
    "        retriever=vectorstore_faiss_aws.as_retriever(), \n",
    "        memory=memory_chain,\n",
    "        #verbose=True,\n",
    "        condense_question_prompt=create_prompt_template(), #CONDENSE_QUESTION_PROMPT, create_prompt_template(), # use the condense prompt template\n",
    "        chain_type='refine', #'map_rerank', #'refine', # s(['stuff', 'map_reduce', 'refine', 'map_rerank'])\n",
    "        max_tokens_limit=100,\n",
    "        get_chat_history=lambda h : h,\n",
    ")  \n",
    "print(\"Starting Refine chat bot\")\n",
    "input_str = ['Enter your query, q to quit']\n",
    "while True:\n",
    "    query = input(str(input_str))\n",
    "    if 'q' == query or 'quit' == query or 'Q' == query:\n",
    "        print(\"Breaking\")\n",
    "        break\n",
    "    else:\n",
    "        result = qa.run({'question':query, 'chat_history':chat_history} )\n",
    "        input_str.append(f\"Question:{query}\\nAI:Answer:{result}\")\n",
    "\n",
    "print(\"Thank you , that was a nice chat !!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1403d1b-dddb-4439-ad0a-ec096c442f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
