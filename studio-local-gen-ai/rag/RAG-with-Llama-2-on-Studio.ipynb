{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8104cd3e-68df-457a-bcc6-f42c3133d861",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Question Answering with Llama 2, LangChain and Pinecone using SageMaker Studio Notebooks for fast experimentation\n",
    "\n",
    "In this notebook, we demonstrate the use of Llama2 text generation combined with the HuggingFace Embedding model to efficiently construct a Retrieval Augmented Generation (RAG) QnA system on Studio Notebooks. This notebook, powered by Pytorch 2.0.0 Image and an ml.g5.2xlarge instance, enables the download of open-source HuggingFace models. These are converted into local LLMs, which we then use to build, experiment with, tune, and deploy the LLM for a RAG application framework. Additionally, we showcase how the PineCone Embedding store can be utilized to archive and retrieve embeddings, integrating it into your RAG workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797e87af-8a8e-4bae-836b-7398a281802f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "    <strong>Kernel:</strong> PyTorch 2.0.0 Python 3.10 GPU Optimized <strong>Instance Type:</strong> ml.g5.2xlarge\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdbbf41-b8ff-4395-bb3a-80717ffc5303",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 01. Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3664a39-3158-447a-88b2-cb5e9a81a8fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Install the required libriaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d771379-e7b4-4fe3-a2b0-64e6c783c3fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "sagemaker>=2.175.0\n",
    "transformers==4.33.0\n",
    "accelerate==0.21.0\n",
    "datasets==2.13.0\n",
    "langchain==0.0.297\n",
    "pypdf>=3.16.3\n",
    "pinecone-client\n",
    "sentence_transformers\n",
    "safetensors>=0.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d06a51b5-f307-48a7-aa0b-5ba2fac44f4f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker>=2.175.0 (from -r requirements.txt (line 1))\n",
      "  Using cached sagemaker-2.192.1-py2.py3-none-any.whl\n",
      "Collecting transformers==4.33.0 (from -r requirements.txt (line 2))\n",
      "  Using cached transformers-4.33.0-py3-none-any.whl (7.6 MB)\n",
      "Collecting accelerate==0.21.0 (from -r requirements.txt (line 3))\n",
      "  Using cached accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "Collecting datasets==2.13.0 (from -r requirements.txt (line 4))\n",
      "  Using cached datasets-2.13.0-py3-none-any.whl (485 kB)\n",
      "Collecting langchain==0.0.297 (from -r requirements.txt (line 5))\n",
      "  Using cached langchain-0.0.297-py3-none-any.whl (1.7 MB)\n",
      "Collecting pypdf>=3.16.3 (from -r requirements.txt (line 6))\n",
      "  Using cached pypdf-3.16.4-py3-none-any.whl (276 kB)\n",
      "Collecting pinecone-client (from -r requirements.txt (line 7))\n",
      "  Using cached pinecone_client-2.2.4-py3-none-any.whl (179 kB)\n",
      "Collecting sentence_transformers (from -r requirements.txt (line 8))\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Collecting safetensors>=0.3.3 (from -r requirements.txt (line 9))\n",
      "  Using cached safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting bitsandbytes==0.40.2 (from -r requirements.txt (line 10))\n",
      "  Using cached bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 2)) (3.12.0)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers==4.33.0->-r requirements.txt (line 2))\n",
      "  Using cached huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 2)) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 2)) (5.4.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.33.0->-r requirements.txt (line 2))\n",
      "  Using cached regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 2)) (2.28.2)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.0->-r requirements.txt (line 2))\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0->-r requirements.txt (line 2)) (4.65.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 3)) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 4)) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 4)) (0.3.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 4)) (2.0.1)\n",
      "Collecting xxhash (from datasets==2.13.0->-r requirements.txt (line 4))\n",
      "  Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 4)) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 4)) (2023.5.0)\n",
      "Collecting aiohttp (from datasets==2.13.0->-r requirements.txt (line 4))\n",
      "  Using cached aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain==0.0.297->-r requirements.txt (line 5))\n",
      "  Using cached SQLAlchemy-2.0.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain==0.0.297->-r requirements.txt (line 5))\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.297->-r requirements.txt (line 5))\n",
      "  Using cached dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.38 (from langchain==0.0.297->-r requirements.txt (line 5))\n",
      "  Using cached langsmith-0.0.43-py3-none-any.whl (40 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4 (from langchain==0.0.297->-r requirements.txt (line 5))\n",
      "  Using cached numexpr-2.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.297->-r requirements.txt (line 5)) (1.10.7)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.297->-r requirements.txt (line 5)) (8.2.2)\n",
      "Collecting attrs<24,>=23.1.0 (from sagemaker>=2.175.0->-r requirements.txt (line 1))\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.175.0->-r requirements.txt (line 1)) (1.26.132)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.175.0->-r requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.175.0->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.175.0->-r requirements.txt (line 1)) (3.20.3)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.175.0->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.175.0->-r requirements.txt (line 1)) (4.13.0)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.175.0->-r requirements.txt (line 1)) (0.3.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.175.0->-r requirements.txt (line 1)) (0.7.5)\n",
      "Collecting pyyaml>=5.1 (from transformers==4.33.0->-r requirements.txt (line 2))\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.175.0->-r requirements.txt (line 1)) (4.17.3)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.175.0->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.175.0->-r requirements.txt (line 1)) (1.7.0)\n",
      "Collecting loguru>=0.5.0 (from pinecone-client->-r requirements.txt (line 7))\n",
      "  Using cached loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.10/site-packages (from pinecone-client->-r requirements.txt (line 7)) (4.5.0)\n",
      "Collecting dnspython>=2.0.0 (from pinecone-client->-r requirements.txt (line 7))\n",
      "  Using cached dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from pinecone-client->-r requirements.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from pinecone-client->-r requirements.txt (line 7)) (1.26.15)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 8)) (0.15.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 8)) (1.2.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 8)) (1.10.1)\n",
      "Collecting nltk (from sentence_transformers->-r requirements.txt (line 8))\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting sentencepiece (from sentence_transformers->-r requirements.txt (line 8))\n",
      "  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0->-r requirements.txt (line 4)) (3.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.13.0->-r requirements.txt (line 4))\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.13.0->-r requirements.txt (line 4))\n",
      "  Using cached yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.13.0->-r requirements.txt (line 4))\n",
      "  Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.13.0->-r requirements.txt (line 4))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.132 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.175.0->-r requirements.txt (line 1)) (1.29.132)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.175.0->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.175.0->-r requirements.txt (line 1)) (0.6.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.297->-r requirements.txt (line 5))\n",
      "  Using cached marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.297->-r requirements.txt (line 5))\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker>=2.175.0->-r requirements.txt (line 1)) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone-client->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0->-r requirements.txt (line 2)) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.297->-r requirements.txt (line 5)) (2.0.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 3)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 3)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.175.0->-r requirements.txt (line 1)) (0.19.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers->-r requirements.txt (line 8)) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers->-r requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.13.0->-r requirements.txt (line 4)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.13.0->-r requirements.txt (line 4)) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.175.0->-r requirements.txt (line 1)) (1.7.6.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.175.0->-r requirements.txt (line 1)) (0.3.2)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker>=2.175.0->-r requirements.txt (line 1)) (21.6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers->-r requirements.txt (line 8)) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers->-r requirements.txt (line 8)) (9.4.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.297->-r requirements.txt (line 5))\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 3)) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.21.0->-r requirements.txt (line 3)) (1.3.0)\n",
      "Installing collected packages: tokenizers, sentencepiece, bitsandbytes, xxhash, SQLAlchemy, safetensors, regex, pyyaml, pypdf, numexpr, mypy-extensions, multidict, marshmallow, loguru, frozenlist, dnspython, attrs, async-timeout, yarl, typing-inspect, pinecone-client, nltk, langsmith, huggingface-hub, aiosignal, transformers, dataclasses-json, aiohttp, accelerate, sentence_transformers, langchain, sagemaker, datasets\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 5.4.1\n",
      "    Uninstalling PyYAML-5.4.1:\n",
      "      Successfully uninstalled PyYAML-5.4.1\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 22.2.0\n",
      "    Uninstalling attrs-22.2.0:\n",
      "      Successfully uninstalled attrs-22.2.0\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.19.0\n",
      "    Uninstalling accelerate-0.19.0:\n",
      "      Successfully uninstalled accelerate-0.19.0\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.154.0\n",
      "    Uninstalling sagemaker-2.154.0:\n",
      "      Successfully uninstalled sagemaker-2.154.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.132 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed SQLAlchemy-2.0.22 accelerate-0.21.0 aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.1.0 bitsandbytes-0.40.2 dataclasses-json-0.6.1 datasets-2.13.0 dnspython-2.4.2 frozenlist-1.4.0 huggingface-hub-0.18.0 langchain-0.0.297 langsmith-0.0.43 loguru-0.7.2 marshmallow-3.20.1 multidict-6.0.4 mypy-extensions-1.0.0 nltk-3.8.1 numexpr-2.8.7 pinecone-client-2.2.4 pypdf-3.16.4 pyyaml-6.0.1 regex-2023.10.3 safetensors-0.4.0 sagemaker-2.192.1 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.33.0 typing-inspect-0.9.0 xxhash-3.4.1 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e1778-d2e5-41a4-b6db-c377cdbafba8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 02. Load Llama-2 7B chat in the notebook for experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa4d6d-7b9b-4b35-a270-c3fed44ce5e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, let's download the Llama-2-7b-chat-hf model from the Hugging Face Hub. Llama 2 models are gated, to get access follow the instructions [here](https://huggingface.co/meta-llama/Llama-2-7b-hf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe79aca-1958-4ce9-9cad-3c24b54dae3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import nvidia\n",
    "# if torch.cuda.is_available():\n",
    "#     cuda_install_dir = '/'.join(nvidia.__file__.split('/')[:-1]) + '/cuda_runtime/lib/'\n",
    "#     os.environ['LD_LIBRARY_PATH'] =  cuda_install_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33848c91-23a5-4af9-bdb1-c4a98eaa8a88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Huggingface API Token: ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "hf_access_token = getpass.getpass(\"Huggingface API Token:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3b0694-aacd-4d7f-9792-ba416dcfdf24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    LlamaTokenizer, \n",
    "    LlamaForCausalLM, \n",
    "    GenerationConfig,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6f9c2-bd28-40ce-89a4-ccc09ea3d941",
   "metadata": {},
   "source": [
    "The following cell takes few minutes to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8f69641-86ac-4532-92d1-19b7ce4b46a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from HuggingFace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:53<00:00, 56.70s/it]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tg_model_id = \"meta-llama/Llama-2-7b-chat-hf\" #the model id in Hugging Face\n",
    "tg_model_path = f\"./tg_model/{tg_model_id}\" #the local directory where the model will be saved\n",
    "\n",
    "if  not (os.path.exists(tg_model_path)) or os.listdir(tg_model_path and tg_model_path) == []:\n",
    "    print(\"Loading model from HuggingFace\")\n",
    "\n",
    "    tg_model = AutoModelForCausalLM.from_pretrained(\n",
    "        tg_model_id, \n",
    "        token=hf_access_token,\n",
    "        do_sample=True, \n",
    "        use_safetensors=True,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    tg_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tg_model_id, \n",
    "        token=hf_access_token\n",
    "    )\n",
    "\n",
    "    tg_model.save_pretrained(\n",
    "        save_directory=tg_model_path, \n",
    "        from_pt=True\n",
    "    )\n",
    "    tg_tokenizer.save_pretrained(\n",
    "        save_directory=tg_model_path, \n",
    "        from_pt=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Loading from model from local directory\")\n",
    "    tg_model = LlamaForCausalLM.from_pretrained(\n",
    "       tg_model_path,\n",
    "       device_map=\"auto\"\n",
    "    )\n",
    "    tg_tokenizer = AutoTokenizer.from_pretrained(tg_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d7126-5d9c-4cc5-abd8-b0e3551dcc12",
   "metadata": {},
   "source": [
    "Check memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9521f112-fb50-4bb7-a0d9-d1c481429665",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 12.613792GB\n",
      "Memory reserved  12.615234GB\n",
      "Max memory reserved: 12.615234GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Memory allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"Memory reserved  %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"Max memory reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d8188-0bf0-44f4-93d9-b2e4ead0bc17",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 03. Simple question-answering using Llama 2 7B chat and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5875641-131d-4ebf-bcef-a50c2834b845",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that the model is available in memory, we can start using it to answer questions. The Llama-2 chat models expect the prompt to follow the below format:\n",
    "\n",
    "    \n",
    "\\<s>[INST] <\\<SYS\\>>\n",
    "\n",
    "{{ system_prompt }}\n",
    "\n",
    "\\<<SYS\\>>\n",
    "\n",
    "{{ user_message }} [/INST]\n",
    "\n",
    "   \n",
    "where\n",
    "- \\<s> - is the beginning of the sequence.\n",
    "- <\\<SYS>> - is the beginning of the system message.\n",
    "- \\<</SYS\\>> - is the end of the system message.\n",
    "- [INST] - is the beginning of the instructions\n",
    "- [/INST] - is the end of the instructions\n",
    "\n",
    "Let's create a recipe based on the above that will helps us define our prompts going forward. For that we will use [PromptTemplate](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/) from langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c4d5e11-2a1c-4981-a16d-a305325d0c34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"<s>[INST] <<SYS>>\\nYou are an assistant for question-answering tasks. You are helpful and friendly. Use the following pieces of retrieved context to answer the query. If you don't know the answer, you just say I don't know. Use three sentences maximum and keep the answer concise.\n",
    "<<SYS>>\\n\n",
    "{context}\\n\n",
    "{question} [/INST]\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(\n",
    "    template=template, \n",
    "    input_variables=['context','question']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d6a863-6d09-43a3-a21d-ec52de136bf6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we test the model on some questions without providing any context. For our tests, we will use questions about the UK Home Office activities in 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f2c9fb93-b816-40a1-a97b-5726a96f400b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"How is UK home office driving down crime in 2023?\"\n",
    "question2= \"How are the British nationality fees changing in 2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7500142a-8cc4-45dd-a0f0-e46abb7d0b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tg_tokenizer.add_special_tokens(\n",
    "    {\"pad_token\": \"[PAD]\"}\n",
    ")\n",
    "tg_tokenizer.padding_side = \"left\"\n",
    "\n",
    "tg_pipe = transformers.pipeline(\n",
    "    task='text-generation',\n",
    "    model=tg_model, \n",
    "    tokenizer=tg_tokenizer,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tg_tokenizer.eos_token_id,\n",
    "    pad_token_id=tg_tokenizer.eos_token_id,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3df8b60c-a237-46ad-9061-36bab130a263",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the latest crime statistics released by the UK Home Office, crime rates in England and Wales have been steadily decreasing over the past few years. In 2023, the overall crime rate decreased by 3.9% compared to the previous year, with some categories of crime experiencing even larger reductions. For example, violent crime fell by 5.3% and burglary dropped by 7.7%. These trends are consistent with the ongoing efforts of the UK Home Office to drive down crime through a combination of law enforcement and crime prevention strategies.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm=HuggingFacePipeline(pipeline=tg_pipe, model_kwargs={'temperature':0.7})\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "no_context_response = llm_chain.predict(context=\"\", question=question)\n",
    "print(no_context_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074dabc6-d77e-4a98-85dc-12514b6b0783",
   "metadata": {},
   "source": [
    "This answer gives us some relevant information about crime in 2023 but it doesn't answer the question about the concrete activities to drive down crime in 2023. To improve it we pass some context. The below is an extract from  the [Home Office Annual Report and Accounts 2022-2023](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1185849/Home_Office_Annual_Report_and_Accounts_22-23.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39727de5-6eef-492c-969c-013a8fe64e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\"Overall crime, excluding fraud and computer crime which has only been counted since\n",
    "2016, is now down by 54% since 2010, with burglary down 55%, robbery down 77%,\n",
    "violence down 46%, theft down 47%, neighbourhood crime down 51%, and criminal\n",
    "damage down 72%. We are rolling out our ambitious programme, Operation Soteria, to\n",
    "transform rape investigations and prosecutions and have brought in a big package of\n",
    "measures for domestic abuse victims. Our new Fraud Strategy will mean this wicked crime\n",
    "is treated like the epidemic it is.\n",
    "We received the Third Volume of the Manchester Arena Inquiry report and published the\n",
    "Terrorism (Protection of Premises) draft Bill. This is also known as Martyn’s Law. It will\n",
    "place on public places a greater duty to protect their visitors.\n",
    "Our efforts to drive down crime have been boosted by recruiting 20,951 additional police\n",
    "officers by March 2023, exceeding our manifesto commitment to recruit an additional\n",
    "20,000 by this date. This brings the total number of police officers in England and Wales to\n",
    "149,566 in March 2023 – the highest on record against a previous peak of 146,030 in\n",
    "2010. Now the police need to ensure they focus on getting the basics right: the highest\n",
    "professional standards and a relentless focus on crime, not politically correct distractions.\n",
    "This means continuing to smash county lines gangs, using proven methods like stop and\n",
    "search, and deploying officers to high-crime areas. The Anti-Social Behaviour Action Plan\n",
    "reflects the fact that there is no such thing as petty crime and that it is easy for areas to\n",
    "slip into degeneracy and misery\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d60d4814-21bc-4448-a08a-0eb87025c219",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The UK Home Office is driving down crime in 2023 through various initiatives and measures. They have seen a 54% reduction in overall crime, excluding fraud and computer crime, since 2010, with specific categories such as burglary, robbery, violence, theft, and criminal damage showing significant decreases. Additionally, they have launched Operation Soteria to transform rape investigations and prosecutions, and have introduced a new Fraud Strategy to tackle this issue. The Home Office has also recruited 20,951 additional police officers by March 2023, exceeding their manifesto commitment, and is focusing on ensuring the police prioritize professional standards and a relentless focus on crime, rather than politically correct distractions. These efforts include continuing to disrupt county lines gangs, using proven methods like stop and search, and deploying officers to high-crime areas.\n"
     ]
    }
   ],
   "source": [
    "context_response = llm_chain.predict(context=context, question=question)\n",
    "print(context_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252fb6a-305b-43fc-9de5-dfba1b6387cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 04. RAG question answering with Llama 2 7B chat, LangChain and Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c762de8c-0a83-4d64-a4cd-f45f7684da69",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "In the above response, the model provides an answer with data from 2023 based on the context we provided. Next we want to scale this approach using __Retrieval Augmented Generation (RAG)__.\n",
    "With RAG, we will ingest external data into our knowledge base and augment the prompt by adding only the data that is relevant to the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc01627-1b6d-4366-9fc9-ab87661f3700",
   "metadata": {
    "tags": []
   },
   "source": [
    "First we download the external files we want to store in our knowledge base locally so than we can quickly iterate if needed. We will use reports published by the UK Home office in 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e840bf42-f473-469f-90ef-039bb67f6cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "files = [\n",
    "    \"https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1185849/Home_Office_Annual_Report_and_Accounts_22-23.pdf\",\n",
    "    \"https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1185949/2023-9-18_Equality_Impact_Assessment_for_Autumn_2023_fee_increases_FINAL.pdf\",\n",
    "]\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "for url in files:\n",
    "    file_path = os.path.join(\"data\", url.rpartition(\"/\")[2])\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1b4d5b-0cfb-422b-aa90-ed481b48194d",
   "metadata": {},
   "source": [
    "After that, we split files into documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fc83614a-a646-4d4b-ae6f-8d5f6964c038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=5,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702ef35-4338-4aa4-9a4a-481a5db011dc",
   "metadata": {},
   "source": [
    "Next, we generate the embeddings for the documents. For that we will use the [bge-small-en](https://huggingface.co/BAAI/bge-small-en) model. We use HuggingFace transfomers to download it to the local directory and load it in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fb59413-1ae8-4213-bd96-434bfc218674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "em_model_name = \"BAAI/bge-small-en\"\n",
    "em_model_path = f\"./em-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3c45182e-e564-438b-b3d4-2741b466a83d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 384)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "em_model = AutoModel.from_pretrained(\n",
    "    em_model_name,\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "em_tokenizer = AutoTokenizer.from_pretrained(em_model_name, device_map=\"cuda\")\n",
    "\n",
    "# save model to disk\n",
    "em_tokenizer.save_pretrained(\n",
    "    save_directory=f\"{em_model_path}/model\", \n",
    "    from_pt=True\n",
    ")\n",
    "\n",
    "em_model.save_pretrained(\n",
    "    save_directory=f\"{em_model_path}/model\", \n",
    "    from_pt=True\n",
    ")\n",
    "em_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c9e884ee-5762-4674-a4fe-8dec36b9cfa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "def tokenize_text(_input, device):\n",
    "    return em_tokenizer(\n",
    "        [_input], \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "# Run embedding task a function with model and text sentences as input\n",
    "def embedding_generator(_input, normalize=True):\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        embedded_output = em_model(\n",
    "            **tokenize_text(\n",
    "                _input, \n",
    "                em_model.device\n",
    "            )\n",
    "        )\n",
    "        sentence_embeddings = embedded_output[0][:, 0]\n",
    "        # normalize embeddings\n",
    "        if normalize:\n",
    "            sentence_embeddings = torch.nn.functional.normalize(\n",
    "                sentence_embeddings, \n",
    "                p=2, \n",
    "                dim=1\n",
    "            )\n",
    "    \n",
    "    return sentence_embeddings[0, :].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1944501e-6b6b-4a03-bf35-db5b59502d20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size of the document ---> 384\n"
     ]
    }
   ],
   "source": [
    "sample_sentence_embedding = embedding_generator(docs[0].page_content)\n",
    "print(f\"Embedding size of the document --->\", len(sample_sentence_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3bd75-c8d8-4c25-afe0-25406249be72",
   "metadata": {},
   "source": [
    "We are now ready to ingest the embeddings into our vector store. In this notebook we will use [Pinecone](https://www.pinecone.io/), however you can replace the below code with that for the vector store of your choice.\n",
    "If you don't have a Pinecone account you can sign up for free to complete this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4fdf57ca-368e-4275-9a7b-f23d6831abb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Pinecone API Key: ········\n",
      "Pinecone Environment: ········\n"
     ]
    }
   ],
   "source": [
    "#enter your Pinecone keys\n",
    "os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\n",
    "os.environ[\"PINECONE_ENV\"] = getpass.getpass(\"Pinecone Environment:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3167c11c-3900-4fb2-88e9-27dabe57186d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initialize Pinecone\n",
    "import pinecone\n",
    "pinecone.init(\n",
    "    api_key = os.environ[\"PINECONE_API_KEY\"],\n",
    "    environment = os.environ[\"PINECONE_ENV\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead9204-a1c6-4e9c-9b83-6608a00e7a54",
   "metadata": {},
   "source": [
    "In Pinecone, we create a new vector search index and ingest the embeddings we created in the previous step. The size of the index is the dimension of our embeddings model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d2ffc6e6-f9b6-45f4-a87c-473f24098368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check if index already exists, if not we create it\n",
    "index_name = \"rag-index\"\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        dimension=len(sample_sentence_embedding),\n",
    "        metric='cosine'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7089eae7-f173-476e-9004-535893cfd2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#insert the embeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "vector_store = Pinecone.from_documents(\n",
    "    docs, \n",
    "    embedding_generator, \n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439c8ed5-dcb2-45ab-9737-c75043848e0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's do a quick test to see if the similarity search is working well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ff8f1253-5cb9-4225-a7fe-31e28fde496f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "society’s response to these crimes.   \n",
      "In 2022 -23, the Home Office , alongside othe r government departments,  has implemented \n",
      "35% of the commitments in both documents. This  activity  includes:   \n",
      "• Deliver ing two phases of the long -term national behaviour change campaign ‘Enough’, \n",
      "which has reached millions across England and Wales   \n",
      "• Increas ing the size of the Children Affected by Domestic Abuse Fund for specialist\n"
     ]
    }
   ],
   "source": [
    "docs = vector_store.similarity_search(question)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23974b7-71dc-4024-9b60-dcbe42dace73",
   "metadata": {
    "tags": []
   },
   "source": [
    "We have Llama-2 chat model in memory and the embeddings inserted in our Pinecone index. To improve the responses of the Llama 2 chat model we bring it alltogether and implement the RAG architecture easily with the Langchain [RetrievalQA](https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa). RetrievalQA augments our initial prompt with the most similar documents from the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e3d4c5f9-5c2d-4272-83b7-f106486da0d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe4e48a-e4e2-4d92-a349-f1df63121b20",
   "metadata": {},
   "source": [
    "And that's it! Let's ask the model again to see if we will get 2023 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "13d80570-d172-42d4-aa89-e3eceb115111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "#helper method to improve the readability of the response\n",
    "def print_response(llm_response):\n",
    "    temp = [textwrap.fill(line, width=100) for line in llm_response['result'].split('\\n')]\n",
    "    response = '\\n'.join(temp)\n",
    "    print(f\"{llm_response['query']}\\n \\n{response}'\\n \\n Source Documents:\")\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "52c7fdf0-8665-431b-a2a9-c6ca97836fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is UK home office driving down crime in 2023?\n",
      " \n",
      "In 2023, the UK Home Office is driving down crime through various initiatives and activities,\n",
      "including:\n",
      "1. Recruiting additional 20,000 police officers across England and Wales by March 2023 to enhance\n",
      "the police force's capacity to respond to crime.\n",
      "2. Providing the police with the necessary resources and tools to tackle the evolving profile of\n",
      "crime.\n",
      "3. Reducing serious violence, homicide, and neighborhood crime by 20% from December 2019 levels by\n",
      "December 2023.\n",
      "4. Reducing drug-related crime over the next three years.\n",
      "The Home Office is also monitoring and analyzing the risks associated with higher levels of\n",
      "inflation and increasing cost of living, which may impact crime rates in the next twelve months,\n",
      "particularly neighborhood crime and domestic abuse.'\n",
      " \n",
      " Source Documents:\n",
      "{'page': 36.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n",
      "{'page': 81.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n",
      "{'page': 26.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print_response(llm_qa_chain(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb6772-54bb-40e4-bd67-fbdc6c896644",
   "metadata": {
    "tags": []
   },
   "source": [
    "The model returns a more informed response with details from 2023 and the pages in the documents from where it acquired the information. \n",
    "\n",
    "Let's try another question. The answer to this one is in a different document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "00db9ed9-4124-41ad-8750-69005b2fbd94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are the British nationality fees changing in 2023\n",
      " \n",
      "Based on the retrieved context, here is the answer to the query:\n",
      "The British nationality fees are proposed to increase in Autumn 2023, with a 20% increase to the\n",
      "Leave to Remain fee. The exact amount of the increase will be laid out in further Regulations in due\n",
      "course.'\n",
      " \n",
      " Source Documents:\n",
      "{'page': 1.0, 'source': 'data/2023-9-18_Equality_Impact_Assessment_for_Autumn_2023_fee_increases_FINAL.pdf'}\n",
      "{'page': 153.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n",
      "{'page': 1.0, 'source': 'data/2023-9-18_Equality_Impact_Assessment_for_Autumn_2023_fee_increases_FINAL.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print_response(llm_qa_chain(question2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c6aafb-8dee-4dfc-b50e-0a4c3a384f7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can continue our experimentation with more files, different model parameters and different questions. Once we have sufficiet confidence in our approach, \n",
    "we can deploy our models to Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13e2e9-4257-4312-9fe2-e4dd581cd238",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 04. Supercharge your applications with GenAI by deploying your models to Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6df67-2b16-454e-af38-885eac2f5cab",
   "metadata": {
    "tags": []
   },
   "source": [
    "First we import the required libraries, and retrieve the IAM role and session we will use for deployment.  To deploy a model to a SageMaker endpoint, we first need to compress the model artifacts and upload the tar.gz file to Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c78cd-d3d3-43f8-9264-4ce4a97d21b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 04a. Deploy Text Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "faa5920f-fbc1-4fae-9952-df84ef211acb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = sagemaker.Session().boto_region_name\n",
    "bucket = sess.default_bucket() # Set a default S3 bucket\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "prefix = 'qa-rag-models-test/rag-blog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f540b9f6-4422-46d8-877b-9effd162b53c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_model_location = f\"s3://{bucket}/{prefix}/llama-2-7B-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ea17fdee-269e-47f6-acb9-f7972258a5e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "llm_path = sagemaker.s3.S3Uploader.upload(tg_model_path, pretrained_model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "186dcc74-8b08-4407-9293-c986178c58f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "djl_properties_filename = \"serving.properties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "450f48fa-80f7-472b-b3fa-2fed59e7600b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile {djl_properties_filename}\n",
    "engine = MPI\n",
    "option.tensor_parallel_degree = 1\n",
    "option.rolling_batch = auto\n",
    "option.max_rolling_batch_size = 64\n",
    "option.model_loading_timeout = 3600\n",
    "option.paged_attention = true\n",
    "option.trust_remote_code = true\n",
    "option.dtype = fp16\n",
    "option.rolling_batch=lmi-dist\n",
    "option.max_rolling_batch_prefill_tokens=1560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8dbac860-dba1-428b-a1d8-78d5476b86b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!echo -n \"option.s3url = $pretrained_model_location\" >> {djl_properties_filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c7856a2f-ed96-46b8-ac22-2bdad4930739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelfile_base_name = f\"local-{tg_model_id.replace('/', '-')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "be5a6648-204c-4ddc-9699-6f453e40f8bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "local-meta-llama-Llama-2-7b-chat-hf/\n",
      "local-meta-llama-Llama-2-7b-chat-hf/serving.properties\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir {modelfile_base_name}\n",
    "!mv serving.properties {modelfile_base_name}/\n",
    "!tar czvf {modelfile_base_name}.tar.gz {modelfile_base_name}/\n",
    "!rm -rf {modelfile_base_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c9e8fef6-ff1c-4318-bea8-8aab69d07cad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "drwxr-xr-x root/root         0 2023-10-14 11:23 local-meta-llama-Llama-2-7b-chat-hf/\n",
      "-rw-r--r-- root/root       399 2023-10-14 11:23 local-meta-llama-Llama-2-7b-chat-hf/serving.properties\n"
     ]
    }
   ],
   "source": [
    "# list out the contents of the tar gz file for validation\n",
    "!tar -ztvf {modelfile_base_name}.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ec63ea6f-83dc-4e25-931f-a44af164e8bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", region=region, version=\"0.23.0\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c75196f0-46b5-47a8-b246-f1fe56233c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-east-1-477886989750/large-model-lmi/artifacts/local-meta-llama-Llama-2-7b-chat-hf.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Upload file and instantiate a new SageMaker Model\n",
    "s3_code_prefix = \"large-model-lmi/artifacts\"\n",
    "\n",
    "code_artifact = sess.upload_data(f\"{modelfile_base_name}.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "531a0809-b888-40dc-b053-23b20090b954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama2_model_name = sagemaker.utils.name_from_base(\n",
    "    f\"{tg_model_id.replace('/', '-')}\"\n",
    ")\n",
    "\n",
    "tg_sm_model = Model(\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=code_artifact,\n",
    "    role=role,\n",
    "    name=llama2_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce3e4f-b0fe-44d9-afe8-477ad042a5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "endpoint_name = f\"ep-{llama2_model_name}\"\n",
    "\n",
    "tg_sm_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=900,\n",
    "    wait=False, # <-- Set to True, if you would prefer to wait for the endpoint to spin up\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "95a750c8-9150-4040-8b59-4763d1827dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name to use ---> ep-meta-llama-Llama-2-7b-chat-hf-2023-10-14-11-23-22-908\n"
     ]
    }
   ],
   "source": [
    "print(f\"Endpoint name to use ---> {tg_sm_model.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5ad5f953-274e-4ede-be23-2e3f0e3caea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=tg_sm_model.endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4796940c-6fa5-4114-abd8-36009fa2a370",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': '\\n\\nThe current president of Brazil is Jair Bolsonaro. He was inaugurated on January 1, 2019, and is serving a four-year term as the 38th president of Brazil. Prior to his'}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(\n",
    "    {\n",
    "        \"inputs\": \"Who is the president of Brazil?\",\n",
    "        \"parameters\": {\"temperature\": 0.1, \"max_new_tokens\": 50}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8cfb35-850a-44f6-8007-d3cc379a9ff2",
   "metadata": {},
   "source": [
    "### 04b. Deploy Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2f4d2f91-8530-4539-82f6-6ed6c506246d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./em-model/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {em_model_path}/model.py\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModel, \n",
    "    AutoTokenizer\n",
    ")\n",
    "from typing import Any, Dict, Tuple\n",
    "import deepspeed\n",
    "import warnings\n",
    "import tarfile\n",
    "\n",
    "model, tokenizer = None, None\n",
    "model_dir = \"./model/\"\n",
    "\n",
    "\n",
    "def get_model(properties):\n",
    "    \n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "    \n",
    "    print(f\"Loading model from {model_dir}\")\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_dir\n",
    "    )\n",
    "    \n",
    "    model = deepspeed.init_inference(\n",
    "        model,\n",
    "        mp_size=properties[\"tensor_parallel_degree\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Loading tokenizer from {model_dir}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def handle(inputs: Input) -> None:\n",
    "    global model, tokenizer\n",
    "    \n",
    "    if not model:\n",
    "        model, tokenizer = get_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "    \n",
    "    data = inputs.get_as_json()\n",
    "    text = data[\"text\"]\n",
    "    \n",
    "    input_tokenized = tokenizer(\n",
    "        [text], \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors='pt'\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    outputs = model(**input_tokenized)\n",
    "    \n",
    "    sentence_embeddings = outputs[0][:, 0]\n",
    "    \n",
    "    # normalize embeddings\n",
    "    sentence_embeddings = torch.nn.functional.normalize(\n",
    "        sentence_embeddings, \n",
    "        p=2, \n",
    "        dim=1\n",
    "    )\n",
    "    sentence_embeddings = sentence_embeddings[0, :].tolist()\n",
    "    \n",
    "    result = {\"outputs\": sentence_embeddings}\n",
    "    \n",
    "    return Output().add(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c2662467-c8a3-42c9-afc4-7b30d983f86c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./em-model/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {em_model_path}/requirements.txt\n",
    "einops\n",
    "git+https://github.com/lanking520/DeepSpeed.git@falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "caf2d608-f949-4f7b-a8d4-e4e14dae864e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./em-model/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile {em_model_path}/serving.properties\n",
    "engine=DeepSpeed\n",
    "option.tensor_parallel_degree=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "fcc3ba20-cc94-4e8b-bb70-cc5482cd73a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "rm: cannot remove 'embeddings-model.tar.gz': No such file or directory\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "./\n",
      "./model.py\n",
      "./requirements.txt\n",
      "./model/\n",
      "./model/pytorch_model.bin\n",
      "./model/tokenizer_config.json\n",
      "./model/config.json\n",
      "./model/special_tokens_map.json\n",
      "./model/vocab.txt\n",
      "./model/tokenizer.json\n",
      "./serving.properties\n"
     ]
    }
   ],
   "source": [
    "!rm embeddings-model.tar.gz\n",
    "!rm -rf {em_model_path}/.ipynb_checkpoints\n",
    "!cd {em_model_path} && tar -czvf ../embeddings-model.tar.gz ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e56b620f-64f2-47ef-8d81-1776079066f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "drwxr-xr-x root/root         0 2023-10-14 11:49 ./\n",
      "-rw-r--r-- root/root      1521 2023-10-14 11:49 ./model.py\n",
      "-rw-r--r-- root/root        62 2023-10-14 11:49 ./requirements.txt\n",
      "drwxr-xr-x root/root         0 2023-10-14 09:35 ./model/\n",
      "-rw-r--r-- root/root 133503977 2023-10-14 10:11 ./model/pytorch_model.bin\n",
      "-rw-r--r-- root/root       390 2023-10-14 10:11 ./model/tokenizer_config.json\n",
      "-rw-r--r-- root/root       701 2023-10-14 10:11 ./model/config.json\n",
      "-rw-r--r-- root/root       125 2023-10-14 10:11 ./model/special_tokens_map.json\n",
      "-rw-r--r-- root/root    231508 2023-10-14 10:11 ./model/vocab.txt\n",
      "-rw-r--r-- root/root    711396 2023-10-14 10:11 ./model/tokenizer.json\n",
      "-rw-r--r-- root/root        49 2023-10-14 11:49 ./serving.properties\n"
     ]
    }
   ],
   "source": [
    "!tar -tzvf embeddings-model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "cf9231f7-cfd0-4cdc-b003-b64b0f6d1f96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\n"
     ]
    }
   ],
   "source": [
    "embedding_inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", region=region, version=\"0.23.0\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "835f406a-77b2-4917-bd13-f2deca1b4e38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-east-1-477886989750/large-model-lmi/artifacts/embeddings-model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "embedded_code_artifact = sess.upload_data(\"embeddings-model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {embedded_code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "398dd9dc-dffc-4ddf-8b6b-3347c46b07e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new model ---> BAAI-bge-small-en-2023-10-14-11-50-19-212\n"
     ]
    }
   ],
   "source": [
    "embedding_model_name = sagemaker.utils.name_from_base(\n",
    "    f\"{em_model_name.replace('/', '-')}\"\n",
    ")\n",
    "\n",
    "em_sm_model = Model(\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=embedding_inference_image_uri,\n",
    "    model_data=embedded_code_artifact,\n",
    "    role=role,\n",
    "    name=embedding_model_name,\n",
    ")\n",
    "print(f\"Creating a new model ---> {em_sm_model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "71dd6a4a-4014-40c1-937c-436a57ee92fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_instance_type = \"ml.g5.2xlarge\"\n",
    "\n",
    "em_sm_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=embedding_instance_type,\n",
    "    endpoint_name=f\"ep-{embedding_model_name}\",\n",
    "    container_startup_health_check_timeout=900,\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f81c1-be49-4789-8dd7-92e1590cd818",
   "metadata": {},
   "source": [
    "## 05. Run LangChain Inference using SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8faab706-c63b-49fe-a3ac-e0718aab3e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.llms import SagemakerEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "889ba435-b220-4ce4-a662-f5d03d32c235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        body = {\n",
    "            \"inputs\": prompt, \n",
    "            \"parameters\": model_kwargs\n",
    "        }\n",
    "        input_str = json.dumps(body)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"generated_text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "05d91047-1aad-4c07-8f84-afe677b3e3e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content_handler = ContentHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "df9aeb46-aeb5-427d-895c-25271b82822e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert your local LLM into SageMaker endpoint LLM\n",
    "llm_sm_ep = SagemakerEndpoint(\n",
    "    endpoint_name=tg_sm_model.endpoint_name, # <--- Your endpoint name\n",
    "    region_name=region,\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.05, \n",
    "        \"max_new_tokens\": 512\n",
    "    },\n",
    "    content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ee6d250c-c9cf-4d43-8f8b-075b13cb3d02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_qa_smep_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm_sm_ep,\n",
    "    chain_type='stuff',\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1390fab0-e427-4264-8cd5-5711f8f762ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is UK home office driving down crime in 2023?\n",
      " \n",
      "The UK Home Office is driving down crime in 2023 through various initiatives and strategies outlined\n",
      "in its Performance Overview and Outcome Delivery Plan. These include:\n",
      "1. Recruiting additional police officers across England and Wales to improve police response and\n",
      "resource allocation.\n",
      "2. Providing the police with the necessary tools and resources to tackle evolving crime profiles.\n",
      "3. Reducing serious violence, homicide, and neighborhood crime by 20% from December 2019 levels by\n",
      "December 2023.\n",
      "4. Reducing drug-related crime over the next three years.\n",
      "The Home Office has also identified risks associated with higher levels of inflation and increasing\n",
      "cost of living, which may lead to an increase in crime over the next twelve months, particularly\n",
      "neighborhood crime and domestic abuse.'\n",
      " \n",
      " Source Documents:\n",
      "{'page': 36.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n",
      "{'page': 81.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n",
      "{'page': 26.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print_response(llm_qa_smep_chain(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "be62da26-9769-48ff-81b3-de5bb1d01703",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are the British nationality fees changing in 2023\n",
      " \n",
      "Based on the provided context, here is the answer to the query:\n",
      "The British nationality fees are proposed to change in 2023, with a 20% increase to the Leave to\n",
      "Remain fee. The exact amount of the increase will be laid out in further Regulations in due course.'\n",
      " \n",
      " Source Documents:\n",
      "{'page': 1.0, 'source': 'data/2023-9-18_Equality_Impact_Assessment_for_Autumn_2023_fee_increases_FINAL.pdf'}\n",
      "{'page': 153.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n",
      "{'page': 1.0, 'source': 'data/2023-9-18_Equality_Impact_Assessment_for_Autumn_2023_fee_increases_FINAL.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print_response(llm_qa_smep_chain(question2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4510777a-8caf-4391-871f-cc3ec1041dab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 05a. Invoke the Embedding Endpoint for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd5bb5-5f12-4025-97da-950a6c7222fe",
   "metadata": {},
   "source": [
    "This section shows you how to invoke your custom embedding endpoint for inference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6b5daf68-2b58-4ad8-9741-dc1b9abb7565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=em_sm_model.endpoint_name,\n",
    "    Body=json.dumps({\n",
    "        \"text\": \"This is a sample text\"\n",
    "    }),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "outputs = json.loads(response_model[\"Body\"].read().decode(\"utf8\"))['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "45c9e4a9-efb8-40c5-b7d1-bd9e52f914a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample embeddings ---> [-0.0482177734375, -0.0160675048828125, 0.00830841064453125, -0.04095458984375, 0.0013589859008789062, 0.014678955078125, 0.0142822265625, 0.051422119140625, 0.0207366943359375, -0.002231597900390625]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample embeddings ---> {outputs[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fb1c0f-0541-4ae0-9c69-83b0e838d47f",
   "metadata": {},
   "source": [
    "## 06. Clean Up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c5cc5-1b43-41c1-934c-579eca55829d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete your text generation endpoint\n",
    "sm_client.delete_endpoint(\n",
    "    EndpointName=tg_sm_model.endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0063170-9664-4043-b0bd-57919088da48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete your text embedding endpoint\n",
    "sm_client.delete_endpoint(\n",
    "    EndpointName=em_sm_model.endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982cf8a-5a11-4088-bc2a-5ec72f4f6d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
