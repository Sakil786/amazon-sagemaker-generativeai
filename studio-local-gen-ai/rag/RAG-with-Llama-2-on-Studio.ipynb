{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "797e87af-8a8e-4bae-836b-7398a281802f",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook was tested in SageMaker Studio with a Data Science 3.0 image on a ml.g5.xlarge instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdbbf41-b8ff-4395-bb3a-80717ffc5303",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 01. Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3664a39-3158-447a-88b2-cb5e9a81a8fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Install the required libriaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d771379-e7b4-4fe3-a2b0-64e6c783c3fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "sagemaker\n",
    "torch==2.0.1\n",
    "git+https://github.com/huggingface/transformers.git\n",
    "accelerate==0.21.0\n",
    "datasets==2.13.0\n",
    "langchain==0.0.297\n",
    "pypdf>=3.8,<4\n",
    "pinecone-client\n",
    "sentence_transformers\n",
    "safetensors>=0.3.3\n",
    "bitsandbytes==0.40.2\n",
    "jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06a51b5-f307-48a7-aa0b-5ba2fac44f4f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git (from -r requirements.txt (line 3))\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-mm3kyjqj\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-mm3kyjqj\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 9ed538f2e67ee10323d96c97284cf83d44f0c507\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.184.0)\n",
      "Collecting sagemaker (from -r requirements.txt (line 1))\n",
      "  Downloading sagemaker-2.188.0.tar.gz (892 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m892.2/892.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch==2.0.1 (from -r requirements.txt (line 2))\n",
      "  Using cached torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "Collecting accelerate==0.21.0 (from -r requirements.txt (line 4))\n",
      "  Obtaining dependency information for accelerate==0.21.0 from https://files.pythonhosted.org/packages/70/f9/c381bcdd0c3829d723aa14eec8e75c6c377b4ca61ec68b8093d9f35fc7a7/accelerate-0.21.0-py3-none-any.whl.metadata\n",
      "  Using cached accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting datasets==2.13.0 (from -r requirements.txt (line 5))\n",
      "  Obtaining dependency information for datasets==2.13.0 from https://files.pythonhosted.org/packages/17/d8/f808e32ed7fa86617b9ac7a37b7dcff894c839108c4871cc33ffc4e65b7d/datasets-2.13.0-py3-none-any.whl.metadata\n",
      "  Using cached datasets-2.13.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting langchain==0.0.297 (from -r requirements.txt (line 6))\n",
      "  Obtaining dependency information for langchain==0.0.297 from https://files.pythonhosted.org/packages/74/34/e1a55be448d4fd1866516b9612097c76c7d384d64490a37677c667de58bc/langchain-0.0.297-py3-none-any.whl.metadata\n",
      "  Using cached langchain-0.0.297-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pypdf<4,>=3.8 (from -r requirements.txt (line 7))\n",
      "  Obtaining dependency information for pypdf<4,>=3.8 from https://files.pythonhosted.org/packages/bf/53/8840f93c5dcd108c02cac7343e194f9dc5d15ade6200ccc661ab4e1352b5/pypdf-3.16.2-py3-none-any.whl.metadata\n",
      "  Downloading pypdf-3.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting pinecone-client (from -r requirements.txt (line 8))\n",
      "  Obtaining dependency information for pinecone-client from https://files.pythonhosted.org/packages/df/d4/cffbb61236c6c1d7510e835c1ff843e4e7d705ed59d21c0e5b6dc1cb4fd8/pinecone_client-2.2.4-py3-none-any.whl.metadata\n",
      "  Using cached pinecone_client-2.2.4-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting sentence_transformers (from -r requirements.txt (line 9))\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Collecting safetensors>=0.3.3 (from -r requirements.txt (line 10))\n",
      "  Obtaining dependency information for safetensors>=0.3.3 from https://files.pythonhosted.org/packages/6c/f0/c17bbdb1e5f9dab29d44cade445135789f75f8f08ea2728d04493ea8412b/safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting bitsandbytes==0.40.2 (from -r requirements.txt (line 11))\n",
      "  Obtaining dependency information for bitsandbytes==0.40.2 from https://files.pythonhosted.org/packages/32/ea/75961538b08e4ed568057198717aabdebeaf6f398b7692420532e6861754/bitsandbytes-0.40.2-py3-none-any.whl.metadata\n",
      "  Using cached bitsandbytes-0.40.2-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (3.1.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r requirements.txt (line 2)) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r requirements.txt (line 2)) (4.3.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r requirements.txt (line 2)) (1.10.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r requirements.txt (line 2)) (2.8.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Collecting triton==2.0.0 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 4)) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 4)) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 4)) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from accelerate==0.21.0->-r requirements.txt (line 4)) (6.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 5)) (13.0.0)\n",
      "Collecting dill<0.3.7,>=0.3.0 (from datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 5)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 5)) (4.64.1)\n",
      "Collecting xxhash (from datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/13/c3/e942893f4864a424514c81640f114980cfd5aff7e7414d1e0255f4571111/xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 5)) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 5)) (2022.7.1)\n",
      "Collecting aiohttp (from datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for huggingface-hub<1.0.0,>=0.11.0 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.297->-r requirements.txt (line 6)) (1.4.39)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for async-timeout<5.0.0,>=4.0.0 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for dataclasses-json<0.7,>=0.5.7 from https://files.pythonhosted.org/packages/21/1f/1cff009cff64420572b9f75b70e4a054095719179a172297dfdd65843162/dataclasses_json-0.6.1-py3-none-any.whl.metadata\n",
      "  Downloading dataclasses_json-0.6.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.38 (from langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for langsmith<0.1.0,>=0.0.38 from https://files.pythonhosted.org/packages/70/31/4bd6640c0e2033849630fefe885430236948c91e3b501fae32705d5118dc/langsmith-0.0.41-py3-none-any.whl.metadata\n",
      "  Downloading langsmith-0.0.41-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4 (from langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for numexpr<3.0.0,>=2.8.4 from https://files.pythonhosted.org/packages/2d/03/de1341ec86bbdf1e4a7ad34d95af4762be8a3efab01d5f96922f1228da3e/numexpr-2.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numexpr-2.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting pydantic<3,>=1 (from langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for pydantic<3,>=1 from https://files.pythonhosted.org/packages/73/66/0a72c9fcde42e5650c8d8d5c5c1873b9a3893018020c77ca8eb62708b923/pydantic-2.4.2-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.4.2-py3-none-any.whl.metadata (158 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.6/158.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tenacity<9.0.0,>=8.1.0 (from langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for tenacity<9.0.0,>=8.1.0 from https://files.pythonhosted.org/packages/f4/f1/990741d5bb2487d529d20a433210ffa136a367751e454214013b441c4575/tenacity-8.2.3-py3-none-any.whl.metadata\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 2)) (68.2.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 2)) (0.41.2)\n",
      "Collecting cmake (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for cmake from https://files.pythonhosted.org/packages/72/89/b1cf3cd5fb9f4ae796dd4a743412553f884dad2acbf6b9828d3a0c2b5524/cmake-3.27.6-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Downloading cmake-3.27.6-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lit (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Downloading lit-17.0.1.tar.gz (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (1.28.42)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (4.24.2)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (4.11.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (0.3.1)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (0.7.5)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (4.19.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (2.5.2)\n",
      "Requirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (1.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0.dev0->-r requirements.txt (line 3)) (2022.7.9)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.0.dev0->-r requirements.txt (line 3))\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/57/bd/45b5ef6b088880779f70acf60027f7043ca5fa1b98f4a4345cf3aea09044/tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting loguru>=0.5.0 (from pinecone-client->-r requirements.txt (line 8))\n",
      "  Obtaining dependency information for loguru>=0.5.0 from https://files.pythonhosted.org/packages/03/0a/4f6fed21aa246c6b49b561ca55facacc2a44b87d65b8b92362a8e99ba202/loguru-0.7.2-py3-none-any.whl.metadata\n",
      "  Using cached loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting dnspython>=2.0.0 (from pinecone-client->-r requirements.txt (line 8))\n",
      "  Obtaining dependency information for dnspython>=2.0.0 from https://files.pythonhosted.org/packages/f6/b4/0a9bee52c50f226a3cbfb54263d02bb421c7f2adc136520729c2c689c1e5/dnspython-2.4.2-py3-none-any.whl.metadata\n",
      "  Using cached dnspython-2.4.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from pinecone-client->-r requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from pinecone-client->-r requirements.txt (line 8)) (2.0.4)\n",
      "Collecting torchvision (from sentence_transformers->-r requirements.txt (line 9))\n",
      "  Using cached torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 9)) (1.0.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 9)) (1.11.2)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 9)) (3.7)\n",
      "Collecting sentencepiece (from sentence_transformers->-r requirements.txt (line 9))\n",
      "  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->-r requirements.txt (line 12)) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0->-r requirements.txt (line 5)) (2.0.4)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Using cached yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.42 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker->-r requirements.txt (line 1)) (1.31.42)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker->-r requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker->-r requirements.txt (line 1)) (0.6.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for marshmallow<4.0.0,>=3.18.0 from https://files.pythonhosted.org/packages/ed/3c/cebfdcad015240014ff08b883d1c0c427f2ba45ae8c6572851b6ef136cad/marshmallow-3.20.1-py3-none-any.whl.metadata\n",
      "  Using cached marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for typing-inspect<1,>=0.4.0 from https://files.pythonhosted.org/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl.metadata\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker->-r requirements.txt (line 1)) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.21.0->-r requirements.txt (line 4)) (3.0.9)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/d8/f0/a2ee543a96cc624c35a9086f39b1ed2aa403c6d355dfe47a11ee5c64a164/annotated_types-0.5.0-py3-none-any.whl.metadata\n",
      "  Using cached annotated_types-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pydantic-core==2.10.1 (from pydantic<3,>=1->langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for pydantic-core==2.10.1 from https://files.pythonhosted.org/packages/86/67/d36e2237d84ac96f400e29586da1e21eabf9aa227fc9c3e4410fbc6408de/pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting typing-extensions (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for typing-extensions from https://files.pythonhosted.org/packages/24/21/7d397a4b7934ff4028987914ac1044d3b7d52712f30e2ac7a2ae5bc86dd0/typing_extensions-4.8.0-py3-none-any.whl.metadata\n",
      "  Using cached typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone-client->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.13.0->-r requirements.txt (line 5)) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.13.0->-r requirements.txt (line 5)) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.297->-r requirements.txt (line 6)) (1.1.1)\n",
      "Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for huggingface-hub<1.0.0,>=0.11.0 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 1)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 1)) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 1)) (0.10.2)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Using cached multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers->-r requirements.txt (line 9)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers->-r requirements.txt (line 9)) (1.3.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.13.0->-r requirements.txt (line 5)) (2022.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.7 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker->-r requirements.txt (line 1)) (1.7.6.7)\n",
      "INFO: pip is looking at multiple versions of pathos to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pathos (from sagemaker->-r requirements.txt (line 1))\n",
      "  Using cached pathos-0.3.0-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker->-r requirements.txt (line 1)) (0.3.3)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker->-r requirements.txt (line 1)) (21.6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers->-r requirements.txt (line 9)) (2.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.1->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers->-r requirements.txt (line 9)) (10.0.0)\n",
      "Collecting urllib3>=1.21.1 (from pinecone-client->-r requirements.txt (line 8))\n",
      "  Obtaining dependency information for urllib3>=1.21.1 from https://files.pythonhosted.org/packages/48/fe/a5c6cc46e9fe9171d7ecf0f33ee7aae14642f8d74baa7af4d7840f9358be/urllib3-1.26.17-py2.py3-none-any.whl.metadata\n",
      "  Downloading urllib3-1.26.17-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m966.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.297->-r requirements.txt (line 6)) (0.4.3)\n",
      "Using cached accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "Using cached datasets-2.13.0-py3-none-any.whl (485 kB)\n",
      "Using cached langchain-0.0.297-py3-none-any.whl (1.7 MB)\n",
      "Using cached bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\n",
      "Downloading pypdf-3.16.2-py3-none-any.whl (276 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.3/276.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pinecone_client-2.2.4-py3-none-any.whl (179 kB)\n",
      "Using cached safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
      "Using cached dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
      "Downloading langsmith-0.0.41-py3-none-any.whl (39 kB)\n",
      "Using cached loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "Downloading numexpr-2.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.1/384.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.4.2-py3-none-any.whl (395 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.8/395.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Using cached xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
      "Downloading urllib3-1.26.17-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "Using cached marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading cmake-3.27.6-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sagemaker, transformers, lit\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.188.0-py2.py3-none-any.whl size=1193902 sha256=90da135e7b04a612590e8baef116d0e1c009060d1a9b4cb64c6e57eb1a5d3350\n",
      "  Stored in directory: /root/.cache/pip/wheels/94/00/3e/af9dde735e7ee826b11724be1de1a009a4179984f236a7a928\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.34.0.dev0-py3-none-any.whl size=7745541 sha256=390739a3a2d32eafea1e50471c75c6eafc277d05c1bb1bd4d6dd1a0e12178d00\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ryl_uh4_/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-17.0.1-py3-none-any.whl size=93254 sha256=8ffb849abb6a44c127733847cf713faba6a67035c352e4f55785efcc00b1207a\n",
      "  Stored in directory: /root/.cache/pip/wheels/cf/3a/a0/f65551951357f983270eb3b210b98c6be543f3ed5cf89deba4\n",
      "Successfully built sagemaker transformers lit\n",
      "Installing collected packages: sentencepiece, safetensors, lit, cmake, bitsandbytes, xxhash, urllib3, typing-extensions, tenacity, pypdf, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numexpr, multidict, loguru, frozenlist, dnspython, dill, async-timeout, annotated-types, yarl, typing-inspect, pydantic-core, nvidia-cusolver-cu11, nvidia-cudnn-cu11, multiprocess, marshmallow, aiosignal, pydantic, pinecone-client, pathos, huggingface-hub, dataclasses-json, aiohttp, tokenizers, langsmith, transformers, sagemaker, langchain, datasets, triton, torch, torchvision, sentence_transformers, accelerate\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.4\n",
      "    Uninstalling urllib3-2.0.4:\n",
      "      Successfully uninstalled urllib3-2.0.4\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.0.1\n",
      "    Uninstalling tenacity-8.0.1:\n",
      "      Successfully uninstalled tenacity-8.0.1\n",
      "  Attempting uninstall: numexpr\n",
      "    Found existing installation: numexpr 2.8.3\n",
      "    Uninstalling numexpr-2.8.3:\n",
      "      Successfully uninstalled numexpr-2.8.3\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.15\n",
      "    Uninstalling multiprocess-0.70.15:\n",
      "      Successfully uninstalled multiprocess-0.70.15\n",
      "  Attempting uninstall: pathos\n",
      "    Found existing installation: pathos 0.3.1\n",
      "    Uninstalling pathos-0.3.1:\n",
      "      Successfully uninstalled pathos-0.3.1\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.184.0\n",
      "    Uninstalling sagemaker-2.184.0:\n",
      "      Successfully uninstalled sagemaker-2.184.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.3 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.2.2 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.15.0 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.0a7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.21.0 aiohttp-3.8.5 aiosignal-1.3.1 annotated-types-0.5.0 async-timeout-4.0.3 bitsandbytes-0.40.2 cmake-3.27.6 dataclasses-json-0.6.1 datasets-2.13.0 dill-0.3.6 dnspython-2.4.2 frozenlist-1.4.0 huggingface-hub-0.16.4 langchain-0.0.297 langsmith-0.0.41 lit-17.0.1 loguru-0.7.2 marshmallow-3.20.1 multidict-6.0.4 multiprocess-0.70.14 numexpr-2.8.7 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pathos-0.3.0 pinecone-client-2.2.4 pydantic-2.4.2 pydantic-core-2.10.1 pypdf-3.16.2 safetensors-0.3.3 sagemaker-2.188.0 sentence_transformers-2.2.2 sentencepiece-0.1.99 tenacity-8.2.3 tokenizers-0.14.0 torch-2.0.1 torchvision-0.15.2 transformers-4.34.0.dev0 triton-2.0.0 typing-extensions-4.8.0 typing-inspect-0.9.0 urllib3-1.26.17 xxhash-3.3.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b62ea-3dc3-4e1a-91c8-1359f112bd38",
   "metadata": {
    "tags": []
   },
   "source": [
    "The below is only needed if using bits and bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c96bb5-a3be-45ce-b38b-95c2523a6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import nvidia\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cuda_install_dir = '/'.join(nvidia.__file__.split('/')[:-1]) + '/cuda_runtime/lib/'\n",
    "    os.environ['LD_LIBRARY_PATH'] =  cuda_install_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e1778-d2e5-41a4-b6db-c377cdbafba8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 02. Load Llama-2 7B chat in the notebook for experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa4d6d-7b9b-4b35-a270-c3fed44ce5e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, let's download the Llama-2-7b-chat-hf model from the Hugging Face Hub. Llama 2 models are gated, to get access follow the instructions [here](https://huggingface.co/meta-llama/Llama-2-7b-hf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3b0694-aacd-4d7f-9792-ba416dcfdf24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, GenerationConfig,AutoModelForCausalLM\n",
    "import transformers\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\" #the model id in Hugging Face\n",
    "model_path = f\"./model/{model_id}\" #the local directory where the model will be saved\n",
    "\n",
    "access_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8f69641-86ac-4532-92d1-19b7ce4b46a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4996a2aad1744580a398251d01b29d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "if  not (os.path.exists(model_path)) or os.listdir(model_path and model_path) == []:\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=10.0)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, \n",
    "        token=access_token, \n",
    "        device_map=\"auto\", \n",
    "        do_sample=True, \n",
    "        use_safetensors=True, \n",
    "    #    quantization_config=quantization_config ,\n",
    "        torch_dtype=torch.float16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)\n",
    "\n",
    "    model.save_pretrained(save_directory=model_path, from_pt=True)\n",
    "    tokenizer.save_pretrained(save_directory=model_path, from_pt=True)\n",
    "else:\n",
    "    model = LlamaForCausalLM.from_pretrained(model_path,device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d7126-5d9c-4cc5-abd8-b0e3551dcc12",
   "metadata": {},
   "source": [
    "Check memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9521f112-fb50-4bb7-a0d9-d1c481429665",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 12.613792GB\n",
      "Memory reserved  12.615234GB\n",
      "Max memory reserved: 12.615234GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Memory allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"Memory reserved  %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"Max memory reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d8188-0bf0-44f4-93d9-b2e4ead0bc17",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 03. Simple question-answering using Llama 2 7B chat and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5875641-131d-4ebf-bcef-a50c2834b845",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that the model is available in memory, we can start using it to answer questions. The Llama-2 chat models expect the prompt to follow the below format:\n",
    "\n",
    "    \n",
    "\\<s>[INST] <\\<SYS\\>>\n",
    "\n",
    "{{ system_prompt }}\n",
    "\n",
    "\\<<SYS\\>>\n",
    "\n",
    "{{ user_message }} [/INST]\n",
    "\n",
    "   \n",
    "where\n",
    "- \\<s> - is the beginning of the sequence.\n",
    "- <\\<SYS>> - is the beginning of the system message.\n",
    "- \\<</SYS\\>> - is the end of the system message.\n",
    "- [INST] - is the beginning of the instructions\n",
    "- [/INST] - is the end of the instructions\n",
    "\n",
    "Let's create a recipe based on the above that will helps us define our prompts going forward. For that we will use [PromptTemplate](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/) from langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c4d5e11-2a1c-4981-a16d-a305325d0c34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"<s>[INST] <<SYS>>\\nYou are an assistant for question-answering tasks. You are helpful and friendly. Use the following pieces of retrieved context to answer the query. If you don't know the answer, you just say I don't know. Use three sentences maximum and keep the answer concise.\n",
    "<<SYS>>\\n\n",
    "{context}\\n\n",
    "{question} [/INST]\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(template=template, input_variables=['context','question'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d6a863-6d09-43a3-a21d-ec52de136bf6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we test the model on some questions without providing any context. For our tests, we will use questions about the UK Home Office activities in 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2c9fb93-b816-40a1-a97b-5726a96f400b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"How is UK home office driving down crime in 2023?\"\n",
    "question2= \"How are the nationality fees changing in 2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7500142a-8cc4-45dd-a0f0-e46abb7d0b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "    task='text-generation',\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=400,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3df8b60c-a237-46ad-9061-36bab130a263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the latest crime statistics released by the UK Home Office, crime in England and Wales has decreased by 8% compared to 2022. This is largely attributed to the ongoing efforts of the police to tackle crime through proactive policing strategies, as well as the introduction of new technologies and initiatives to improve crime detection and prevention. Additionally, the Home Office has implemented various measures to support victims of crime and improve the criminal justice system as a whole.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0.7})\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "llm_chain.predict(context=\"\", question=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074dabc6-d77e-4a98-85dc-12514b6b0783",
   "metadata": {},
   "source": [
    "Although this answer is inline with what was asked it doesn't provide a lot of detail on the specific activities that took place in 2023. To improve it we pass some context. The below is an extract from  the [Home Office Annual Report and Accounts 2022-2023](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1185849/Home_Office_Annual_Report_and_Accounts_22-23.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39727de5-6eef-492c-969c-013a8fe64e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\"Overall crime, excluding fraud and computer crime which has only been counted since\n",
    "2016, is now down by 54% since 2010, with burglary down 55%, robbery down 77%,\n",
    "violence down 46%, theft down 47%, neighbourhood crime down 51%, and criminal\n",
    "damage down 72%. We are rolling out our ambitious programme, Operation Soteria, to\n",
    "transform rape investigations and prosecutions and have brought in a big package of\n",
    "measures for domestic abuse victims. Our new Fraud Strategy will mean this wicked crime\n",
    "is treated like the epidemic it is.\n",
    "We received the Third Volume of the Manchester Arena Inquiry report and published the\n",
    "Terrorism (Protection of Premises) draft Bill. This is also known as Martyn’s Law. It will\n",
    "place on public places a greater duty to protect their visitors.\n",
    "Our efforts to drive down crime have been boosted by recruiting 20,951 additional police\n",
    "officers by March 2023, exceeding our manifesto commitment to recruit an additional\n",
    "20,000 by this date. This brings the total number of police officers in England and Wales to\n",
    "149,566 in March 2023 – the highest on record against a previous peak of 146,030 in\n",
    "2010. Now the police need to ensure they focus on getting the basics right: the highest\n",
    "professional standards and a relentless focus on crime, not politically correct distractions.\n",
    "This means continuing to smash county lines gangs, using proven methods like stop and\n",
    "search, and deploying officers to high-crime areas. The Anti-Social Behaviour Action Plan\n",
    "reflects the fact that there is no such thing as petty crime and that it is easy for areas to\n",
    "slip into degeneracy and misery\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d60d4814-21bc-4448-a08a-0eb87025c219",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The UK Home Office is driving down crime in 2023 through various initiatives, including the rollout of Operation Soteria to transform rape investigations and prosecutions, a new Fraud Strategy to tackle this \"epidemic,\" and the recruitment of 20,951 additional police officers by March 2023, exceeding their manifesto commitment. They are also focusing on getting the basics right by ensuring police officers maintain high professional standards and focus on crime, rather than politically correct distractions. This includes continuing to disrupt county lines gangs, using proven methods like stop and search, and deploying officers to high-crime areas.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0.7})\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "llm_chain.predict(context=context, question=question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252fb6a-305b-43fc-9de5-dfba1b6387cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 04. RAG question answering with Llama 2 7B chat, LangChain and Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c762de8c-0a83-4d64-a4cd-f45f7684da69",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "In the above response, the model provides an answer with data from 2023 based on the context we provided. Next we want to scale this approach using __Retrieval Augmented Generation (RAG)__.\n",
    "With RAG, we will ingest external data into our knowledge base and augment the prompt by adding only the data that is relevant to the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc01627-1b6d-4366-9fc9-ab87661f3700",
   "metadata": {
    "tags": []
   },
   "source": [
    "First we download the external files we want to store in our knowledge base locally so than we can quickly iterate if needed. We will use reports published by the UK Home office in 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e840bf42-f473-469f-90ef-039bb67f6cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "files = [\n",
    "    \"https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1185849/Home_Office_Annual_Report_and_Accounts_22-23.pdf\",\n",
    "    \"https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1185949/2023-9-18_Equality_Impact_Assessment_for_Autumn_2023_fee_increases_FINAL.pdf\",\n",
    "]\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "for url in files:\n",
    "    file_path = os.path.join(\"data\", url.rpartition(\"/\")[2])\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1b4d5b-0cfb-422b-aa90-ed481b48194d",
   "metadata": {},
   "source": [
    "After that, we split files into documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc83614a-a646-4d4b-ae6f-8d5f6964c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 50,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702ef35-4338-4aa4-9a4a-481a5db011dc",
   "metadata": {},
   "source": [
    "Next, we generate the embeddings for the documents. For that we will use the [bge-small-en](https://huggingface.co/BAAI/bge-small-en) model. We can do that easily in our notebook through the HuggingFaceBgeEmbeddings class in langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fb59413-1ae8-4213-bd96-434bfc218674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe3504-7bae-46f1-a0aa-360238e5cb0e",
   "metadata": {},
   "source": [
    "The dimension for our embeddings model is 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a183a3c-4e56-42a0-aa37-468f0a438977",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the embedding:  384\n"
     ]
    }
   ],
   "source": [
    "#optionally take a look at one of the embeddings\n",
    "sample_embedding = np.array(embeddings.embed_query(docs[0].page_content))\n",
    "print(\"Size of the embedding: \", sample_embedding.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3bd75-c8d8-4c25-afe0-25406249be72",
   "metadata": {},
   "source": [
    "We are now ready to ingest the embeddings into our vector store. In this notebook we will use [Pinecone](https://www.pinecone.io/), however you can replace the below code with that for the vector store of your choice.\n",
    "If you don't have a Pinecone account you can sign up for free to complete this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fdf57ca-368e-4275-9a7b-f23d6831abb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Pinecone API Key: ········\n",
      "Pinecone Environment: ········\n"
     ]
    }
   ],
   "source": [
    "#enter your Pinecone keys\n",
    "import getpass\n",
    "os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\n",
    "os.environ[\"PINECONE_ENV\"] = getpass.getpass(\"Pinecone Environment:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3167c11c-3900-4fb2-88e9-27dabe57186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize Pinecone\n",
    "import pinecone\n",
    "pinecone.init(\n",
    "    api_key = os.environ[\"PINECONE_API_KEY\"],\n",
    "    environment = os.environ[\"PINECONE_ENV\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead9204-a1c6-4e9c-9b83-6608a00e7a54",
   "metadata": {},
   "source": [
    "In Pinecone, we create a new vector search index and ingest the embeddings we created in the previous step. The size of the index is the dimension of our embeddings model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2ffc6e6-f9b6-45f4-a87c-473f24098368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if index already exists, if not we create it\n",
    "index_name = \"qa-index\"\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric='cosine'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7089eae7-f173-476e-9004-535893cfd2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#insert the embeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "vector_store = Pinecone.from_documents(docs, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439c8ed5-dcb2-45ab-9737-c75043848e0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's do a quick test to see if the similarity search is working well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff8f1253-5cb9-4225-a7fe-31e28fde496f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "£140  million to support victims. These plans and strategies aim to transform the whole of \n",
      "society’s response to these crimes.   \n",
      "In 2022 -23, the Home Office , alongside othe r government departments,  has implemented \n",
      "35% of the commitments in both documents. This  activity  includes:   \n",
      "• Deliver ing two phases of the long -term national behaviour change campaign ‘Enough’, \n",
      "which has reached millions across England and Wales\n"
     ]
    }
   ],
   "source": [
    "docs = vector_store.similarity_search(question)\n",
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23974b7-71dc-4024-9b60-dcbe42dace73",
   "metadata": {
    "tags": []
   },
   "source": [
    "We have Llama-2 chat model in memory and the embeddings inserted in our Pinecone index. To improve the responses of the Llama 2 chat model we bring it alltogether and implement the RAG architecture easily with the Langchain [RetrievalQA](https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa). RetrievalQA augments our initial prompt with the most similar documents from the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3d4c5f9-5c2d-4272-83b7-f106486da0d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm_qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                     chain_type='stuff',\n",
    "                                     retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "                                     return_source_documents=True,\n",
    "                                     chain_type_kwargs={\"prompt\": prompt_template})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe4e48a-e4e2-4d92-a349-f1df63121b20",
   "metadata": {},
   "source": [
    "And that's it! Let's ask the model again to see if we will get 2023 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13d80570-d172-42d4-aa89-e3eceb115111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_response(llm_response):\n",
    "    temp = [textwrap.fill(line, width=100) for line in llm_response['result'].split('\\n')]\n",
    "    response = '\\n'.join(temp)\n",
    "    print(f\"{llm_response['query']}\\n \\n{response}'\\n \\n Source Documents:\")\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52c7fdf0-8665-431b-a2a9-c6ca97836fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is UK home office driving down crime in 2023?\n",
      " \n",
      "The UK Home Office has implemented 35% of the commitments in its plans and strategies to drive down\n",
      "crime in 2023, including delivering two phases of the long-term national behaviour change campaign\n",
      "'Enough' which has reached millions across England and Wales. The campaign aims to transform\n",
      "society's response to crime and support victims. While I don't have access to real-time crime\n",
      "statistics, these efforts suggest a concerted effort to address crime and improve the justice system\n",
      "in the UK.'\n",
      " \n",
      " Source Documents:\n",
      "{'page': 36.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n",
      "{'page': 36.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n",
      "{'page': 36.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print_response(llm_qa_chain(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb6772-54bb-40e4-bd67-fbdc6c896644",
   "metadata": {
    "tags": []
   },
   "source": [
    "The model returns a more informed response with details from 2023 and the pages in the documents from where it acquired the information. \n",
    "\n",
    "Let's try another question. The answer to this one is in a different document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00db9ed9-4124-41ad-8750-69005b2fbd94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are the nationality fees changing in 2023\n",
      " \n",
      "According to the document, the department is proposing changes to immigration and nationality fees\n",
      "in Autumn 2023, with the objective of increasing the level of income generated from those fees to\n",
      "mitigate wider costs. The document does not provide specific information on how nationality fees are\n",
      "changing in 2023. Therefore, I cannot provide an answer to your question.'\n",
      " \n",
      " Source Documents:\n",
      "{'page': 1.0, 'source': 'data/2023-9-18_Equality_Impact_Assessment_for_Autumn_2023_fee_increases_FINAL.pdf'}\n",
      "{'page': 1.0, 'source': 'data/2023-9-18_Equality_Impact_Assessment_for_Autumn_2023_fee_increases_FINAL.pdf'}\n",
      "{'page': 1.0, 'source': 'data/2023-9-18_Equality_Impact_Assessment_for_Autumn_2023_fee_increases_FINAL.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print_response(llm_qa_chain(question2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c6aafb-8dee-4dfc-b50e-0a4c3a384f7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can continue our experimentation with more files, different model parameters and different questions. Once we have sufficiet confidence in our approach, \n",
    "we can deploy our models to Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13e2e9-4257-4312-9fe2-e4dd581cd238",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 04. Supercharge your applications with GenAI by deploying your models to Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6df67-2b16-454e-af38-885eac2f5cab",
   "metadata": {
    "tags": []
   },
   "source": [
    "First we import the required libraries, and retrieve the IAM role and session we will use for deployment.  To deploy a model to a SageMaker endpoint, we first need to compress the model artifacts and upload the tar.gz file to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "faa5920f-fbc1-4fae-9952-df84ef211acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = sagemaker.Session().boto_region_name\n",
    "bucket = sess.default_bucket() # Set a default S3 bucket\n",
    "prefix = 'qa-rag-models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f540b9f6-4422-46d8-877b-9effd162b53c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_model_location = f\"s3://{bucket}/{prefix}/llama-2-7B-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ea17fdee-269e-47f6-acb9-f7972258a5e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "llm_path = sagemaker.s3.S3Uploader.upload(model_path, pretrained_model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "450f48fa-80f7-472b-b3fa-2fed59e7600b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/serving.properties\n",
    "engine = MPI\n",
    "option.tensor_parallel_degree = 1\n",
    "option.rolling_batch = auto\n",
    "option.max_rolling_batch_size = 8\n",
    "option.model_loading_timeout = 3600\n",
    "option.model_id = {{pretrained_model_location}}\n",
    "option.paged_attention = true\n",
    "option.trust_remote_code = true\n",
    "option.dtype = fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8dbac860-dba1-428b-a1d8-78d5476b86b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t\u001b[36mengine\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mMPI\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     2\t\u001b[36moption.tensor_parallel_degree\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     3\t\u001b[36moption.rolling_batch\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     4\t\u001b[36moption.max_rolling_batch_size\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33m8\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     5\t\u001b[36moption.model_loading_timeout\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33m3600\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     6\t\u001b[36moption.model_id\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     7\t\u001b[36moption.paged_attention\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     8\t\u001b[36moption.trust_remote_code\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     9\t\u001b[36moption.dtype\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mfp16\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "import jinja2\n",
    "jinja_env = jinja2.Environment()\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "template = jinja_env.from_string(Path(\"src/serving.properties\").open().read())\n",
    "Path(\"src/serving.properties\").open(\"w\").write(\n",
    "    template.render(s3url=pretrained_model_location)\n",
    ")\n",
    "!pygmentize src/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "be5a6648-204c-4ddc-9699-6f453e40f8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src/\n",
      "src/.ipynb_checkpoints/\n",
      "src/.ipynb_checkpoints/serving-checkpoint.properties\n",
      "src/serving.properties\n"
     ]
    }
   ],
   "source": [
    "!rm model.tar.gz\n",
    "!tar czvf model.tar.gz src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "74576773-8a96-4bec-8d2a-896a48a8ba38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, f\"{bucket}/{prefix}/code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ec63ea6f-83dc-4e25-931f-a44af164e8bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", region=region, version=\"0.23.0\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "18e68d8f-3040-40fb-8930-4dcbf017d2a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "model = Model(image_uri=inference_image_uri, model_data=s3_code_artifact, role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "52ce3e4f-b0fe-44d9-afe8-477ad042a5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = sagemaker.utils.name_from_base(\"llama-2-7b-chat\")\n",
    "model.deploy(initial_instance_count=1, instance_type=\"ml.g5.2xlarge\", endpoint_name=endpoint_name, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5ad5f953-274e-4ede-be23-2e3f0e3caea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = sagemaker.Predictor(\n",
    "        endpoint_name=\"llama-2-7b-chat-2023-09-23-13-43-51-480\",\n",
    "        sagemaker_session=sess,\n",
    "        serializer=serializers.JSONSerializer(),\n",
    "        deserializer=deserializers.JSONDeserializer(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4796940c-6fa5-4114-abd8-36009fa2a370",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': ''}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict({\n",
    "\t\"inputs\": question,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ae544-65a2-4d7a-8760-6f1ae5ec84c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
