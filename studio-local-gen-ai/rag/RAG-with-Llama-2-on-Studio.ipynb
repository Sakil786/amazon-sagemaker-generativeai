{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "797e87af-8a8e-4bae-836b-7398a281802f",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook was tested in SageMaker Studio with a Data Science 3.0 image on a ml.g5.xlarge instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdbbf41-b8ff-4395-bb3a-80717ffc5303",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 01. Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3664a39-3158-447a-88b2-cb5e9a81a8fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Install the required libriaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d771379-e7b4-4fe3-a2b0-64e6c783c3fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "sagemaker>=2.175.0\n",
    "torch==2.0.1\n",
    "git+https://github.com/huggingface/transformers.git\n",
    "accelerate==0.21.0\n",
    "datasets==2.13.0\n",
    "langchain==0.0.297\n",
    "pypdf>=3.8,<4\n",
    "pinecone-client\n",
    "sentence_transformers\n",
    "safetensors>=0.3.3\n",
    "bitsandbytes==0.40.2\n",
    "jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06a51b5-f307-48a7-aa0b-5ba2fac44f4f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git (from -r requirements.txt (line 3))\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-mm3kyjqj\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-mm3kyjqj\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 9ed538f2e67ee10323d96c97284cf83d44f0c507\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.184.0)\n",
      "Collecting sagemaker (from -r requirements.txt (line 1))\n",
      "  Downloading sagemaker-2.188.0.tar.gz (892 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m892.2/892.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch==2.0.1 (from -r requirements.txt (line 2))\n",
      "  Using cached torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "Collecting accelerate==0.21.0 (from -r requirements.txt (line 4))\n",
      "  Obtaining dependency information for accelerate==0.21.0 from https://files.pythonhosted.org/packages/70/f9/c381bcdd0c3829d723aa14eec8e75c6c377b4ca61ec68b8093d9f35fc7a7/accelerate-0.21.0-py3-none-any.whl.metadata\n",
      "  Using cached accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting datasets==2.13.0 (from -r requirements.txt (line 5))\n",
      "  Obtaining dependency information for datasets==2.13.0 from https://files.pythonhosted.org/packages/17/d8/f808e32ed7fa86617b9ac7a37b7dcff894c839108c4871cc33ffc4e65b7d/datasets-2.13.0-py3-none-any.whl.metadata\n",
      "  Using cached datasets-2.13.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting langchain==0.0.297 (from -r requirements.txt (line 6))\n",
      "  Obtaining dependency information for langchain==0.0.297 from https://files.pythonhosted.org/packages/74/34/e1a55be448d4fd1866516b9612097c76c7d384d64490a37677c667de58bc/langchain-0.0.297-py3-none-any.whl.metadata\n",
      "  Using cached langchain-0.0.297-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pypdf<4,>=3.8 (from -r requirements.txt (line 7))\n",
      "  Obtaining dependency information for pypdf<4,>=3.8 from https://files.pythonhosted.org/packages/bf/53/8840f93c5dcd108c02cac7343e194f9dc5d15ade6200ccc661ab4e1352b5/pypdf-3.16.2-py3-none-any.whl.metadata\n",
      "  Downloading pypdf-3.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting pinecone-client (from -r requirements.txt (line 8))\n",
      "  Obtaining dependency information for pinecone-client from https://files.pythonhosted.org/packages/df/d4/cffbb61236c6c1d7510e835c1ff843e4e7d705ed59d21c0e5b6dc1cb4fd8/pinecone_client-2.2.4-py3-none-any.whl.metadata\n",
      "  Using cached pinecone_client-2.2.4-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting sentence_transformers (from -r requirements.txt (line 9))\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Collecting safetensors>=0.3.3 (from -r requirements.txt (line 10))\n",
      "  Obtaining dependency information for safetensors>=0.3.3 from https://files.pythonhosted.org/packages/6c/f0/c17bbdb1e5f9dab29d44cade445135789f75f8f08ea2728d04493ea8412b/safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting bitsandbytes==0.40.2 (from -r requirements.txt (line 11))\n",
      "  Obtaining dependency information for bitsandbytes==0.40.2 from https://files.pythonhosted.org/packages/32/ea/75961538b08e4ed568057198717aabdebeaf6f398b7692420532e6861754/bitsandbytes-0.40.2-py3-none-any.whl.metadata\n",
      "  Using cached bitsandbytes-0.40.2-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (3.1.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r requirements.txt (line 2)) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r requirements.txt (line 2)) (4.3.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r requirements.txt (line 2)) (1.10.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->-r requirements.txt (line 2)) (2.8.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Collecting triton==2.0.0 (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 4)) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 4)) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->-r requirements.txt (line 4)) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from accelerate==0.21.0->-r requirements.txt (line 4)) (6.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 5)) (13.0.0)\n",
      "Collecting dill<0.3.7,>=0.3.0 (from datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 5)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 5)) (4.64.1)\n",
      "Collecting xxhash (from datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/13/c3/e942893f4864a424514c81640f114980cfd5aff7e7414d1e0255f4571111/xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 5)) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 5)) (2022.7.1)\n",
      "Collecting aiohttp (from datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for huggingface-hub<1.0.0,>=0.11.0 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.297->-r requirements.txt (line 6)) (1.4.39)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for async-timeout<5.0.0,>=4.0.0 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for dataclasses-json<0.7,>=0.5.7 from https://files.pythonhosted.org/packages/21/1f/1cff009cff64420572b9f75b70e4a054095719179a172297dfdd65843162/dataclasses_json-0.6.1-py3-none-any.whl.metadata\n",
      "  Downloading dataclasses_json-0.6.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.38 (from langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for langsmith<0.1.0,>=0.0.38 from https://files.pythonhosted.org/packages/70/31/4bd6640c0e2033849630fefe885430236948c91e3b501fae32705d5118dc/langsmith-0.0.41-py3-none-any.whl.metadata\n",
      "  Downloading langsmith-0.0.41-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4 (from langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for numexpr<3.0.0,>=2.8.4 from https://files.pythonhosted.org/packages/2d/03/de1341ec86bbdf1e4a7ad34d95af4762be8a3efab01d5f96922f1228da3e/numexpr-2.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numexpr-2.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting pydantic<3,>=1 (from langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for pydantic<3,>=1 from https://files.pythonhosted.org/packages/73/66/0a72c9fcde42e5650c8d8d5c5c1873b9a3893018020c77ca8eb62708b923/pydantic-2.4.2-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.4.2-py3-none-any.whl.metadata (158 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.6/158.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tenacity<9.0.0,>=8.1.0 (from langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for tenacity<9.0.0,>=8.1.0 from https://files.pythonhosted.org/packages/f4/f1/990741d5bb2487d529d20a433210ffa136a367751e454214013b441c4575/tenacity-8.2.3-py3-none-any.whl.metadata\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 2)) (68.2.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 2)) (0.41.2)\n",
      "Collecting cmake (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for cmake from https://files.pythonhosted.org/packages/72/89/b1cf3cd5fb9f4ae796dd4a743412553f884dad2acbf6b9828d3a0c2b5524/cmake-3.27.6-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Downloading cmake-3.27.6-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lit (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Downloading lit-17.0.1.tar.gz (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (1.28.42)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (4.24.2)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (4.11.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (0.3.1)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (0.7.5)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (4.19.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (2.5.2)\n",
      "Requirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker->-r requirements.txt (line 1)) (1.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0.dev0->-r requirements.txt (line 3)) (2022.7.9)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.0.dev0->-r requirements.txt (line 3))\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/57/bd/45b5ef6b088880779f70acf60027f7043ca5fa1b98f4a4345cf3aea09044/tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting loguru>=0.5.0 (from pinecone-client->-r requirements.txt (line 8))\n",
      "  Obtaining dependency information for loguru>=0.5.0 from https://files.pythonhosted.org/packages/03/0a/4f6fed21aa246c6b49b561ca55facacc2a44b87d65b8b92362a8e99ba202/loguru-0.7.2-py3-none-any.whl.metadata\n",
      "  Using cached loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting dnspython>=2.0.0 (from pinecone-client->-r requirements.txt (line 8))\n",
      "  Obtaining dependency information for dnspython>=2.0.0 from https://files.pythonhosted.org/packages/f6/b4/0a9bee52c50f226a3cbfb54263d02bb421c7f2adc136520729c2c689c1e5/dnspython-2.4.2-py3-none-any.whl.metadata\n",
      "  Using cached dnspython-2.4.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from pinecone-client->-r requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from pinecone-client->-r requirements.txt (line 8)) (2.0.4)\n",
      "Collecting torchvision (from sentence_transformers->-r requirements.txt (line 9))\n",
      "  Using cached torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 9)) (1.0.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 9)) (1.11.2)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->-r requirements.txt (line 9)) (3.7)\n",
      "Collecting sentencepiece (from sentence_transformers->-r requirements.txt (line 9))\n",
      "  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->-r requirements.txt (line 12)) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0->-r requirements.txt (line 5)) (2.0.4)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Using cached yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.42 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker->-r requirements.txt (line 1)) (1.31.42)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker->-r requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker->-r requirements.txt (line 1)) (0.6.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for marshmallow<4.0.0,>=3.18.0 from https://files.pythonhosted.org/packages/ed/3c/cebfdcad015240014ff08b883d1c0c427f2ba45ae8c6572851b6ef136cad/marshmallow-3.20.1-py3-none-any.whl.metadata\n",
      "  Using cached marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for typing-inspect<1,>=0.4.0 from https://files.pythonhosted.org/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl.metadata\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker->-r requirements.txt (line 1)) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.21.0->-r requirements.txt (line 4)) (3.0.9)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/d8/f0/a2ee543a96cc624c35a9086f39b1ed2aa403c6d355dfe47a11ee5c64a164/annotated_types-0.5.0-py3-none-any.whl.metadata\n",
      "  Using cached annotated_types-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pydantic-core==2.10.1 (from pydantic<3,>=1->langchain==0.0.297->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for pydantic-core==2.10.1 from https://files.pythonhosted.org/packages/86/67/d36e2237d84ac96f400e29586da1e21eabf9aa227fc9c3e4410fbc6408de/pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting typing-extensions (from torch==2.0.1->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for typing-extensions from https://files.pythonhosted.org/packages/24/21/7d397a4b7934ff4028987914ac1044d3b7d52712f30e2ac7a2ae5bc86dd0/typing_extensions-4.8.0-py3-none-any.whl.metadata\n",
      "  Using cached typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone-client->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.13.0->-r requirements.txt (line 5)) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.13.0->-r requirements.txt (line 5)) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.297->-r requirements.txt (line 6)) (1.1.1)\n",
      "Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for huggingface-hub<1.0.0,>=0.11.0 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 1)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 1)) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker->-r requirements.txt (line 1)) (0.10.2)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.13.0->-r requirements.txt (line 5))\n",
      "  Using cached multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers->-r requirements.txt (line 9)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers->-r requirements.txt (line 9)) (1.3.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.13.0->-r requirements.txt (line 5)) (2022.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.7 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker->-r requirements.txt (line 1)) (1.7.6.7)\n",
      "INFO: pip is looking at multiple versions of pathos to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pathos (from sagemaker->-r requirements.txt (line 1))\n",
      "  Using cached pathos-0.3.0-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker->-r requirements.txt (line 1)) (0.3.3)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker->-r requirements.txt (line 1)) (21.6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers->-r requirements.txt (line 9)) (2.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.1->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers->-r requirements.txt (line 9)) (10.0.0)\n",
      "Collecting urllib3>=1.21.1 (from pinecone-client->-r requirements.txt (line 8))\n",
      "  Obtaining dependency information for urllib3>=1.21.1 from https://files.pythonhosted.org/packages/48/fe/a5c6cc46e9fe9171d7ecf0f33ee7aae14642f8d74baa7af4d7840f9358be/urllib3-1.26.17-py2.py3-none-any.whl.metadata\n",
      "  Downloading urllib3-1.26.17-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m966.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.297->-r requirements.txt (line 6)) (0.4.3)\n",
      "Using cached accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "Using cached datasets-2.13.0-py3-none-any.whl (485 kB)\n",
      "Using cached langchain-0.0.297-py3-none-any.whl (1.7 MB)\n",
      "Using cached bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\n",
      "Downloading pypdf-3.16.2-py3-none-any.whl (276 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.3/276.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pinecone_client-2.2.4-py3-none-any.whl (179 kB)\n",
      "Using cached safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
      "Using cached dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
      "Downloading langsmith-0.0.41-py3-none-any.whl (39 kB)\n",
      "Using cached loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "Downloading numexpr-2.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.1/384.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.4.2-py3-none-any.whl (395 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.8/395.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Using cached xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
      "Downloading urllib3-1.26.17-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "Using cached marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading cmake-3.27.6-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sagemaker, transformers, lit\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.188.0-py2.py3-none-any.whl size=1193902 sha256=90da135e7b04a612590e8baef116d0e1c009060d1a9b4cb64c6e57eb1a5d3350\n",
      "  Stored in directory: /root/.cache/pip/wheels/94/00/3e/af9dde735e7ee826b11724be1de1a009a4179984f236a7a928\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.34.0.dev0-py3-none-any.whl size=7745541 sha256=390739a3a2d32eafea1e50471c75c6eafc277d05c1bb1bd4d6dd1a0e12178d00\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ryl_uh4_/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-17.0.1-py3-none-any.whl size=93254 sha256=8ffb849abb6a44c127733847cf713faba6a67035c352e4f55785efcc00b1207a\n",
      "  Stored in directory: /root/.cache/pip/wheels/cf/3a/a0/f65551951357f983270eb3b210b98c6be543f3ed5cf89deba4\n",
      "Successfully built sagemaker transformers lit\n",
      "Installing collected packages: sentencepiece, safetensors, lit, cmake, bitsandbytes, xxhash, urllib3, typing-extensions, tenacity, pypdf, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numexpr, multidict, loguru, frozenlist, dnspython, dill, async-timeout, annotated-types, yarl, typing-inspect, pydantic-core, nvidia-cusolver-cu11, nvidia-cudnn-cu11, multiprocess, marshmallow, aiosignal, pydantic, pinecone-client, pathos, huggingface-hub, dataclasses-json, aiohttp, tokenizers, langsmith, transformers, sagemaker, langchain, datasets, triton, torch, torchvision, sentence_transformers, accelerate\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.4\n",
      "    Uninstalling urllib3-2.0.4:\n",
      "      Successfully uninstalled urllib3-2.0.4\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.0.1\n",
      "    Uninstalling tenacity-8.0.1:\n",
      "      Successfully uninstalled tenacity-8.0.1\n",
      "  Attempting uninstall: numexpr\n",
      "    Found existing installation: numexpr 2.8.3\n",
      "    Uninstalling numexpr-2.8.3:\n",
      "      Successfully uninstalled numexpr-2.8.3\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.15\n",
      "    Uninstalling multiprocess-0.70.15:\n",
      "      Successfully uninstalled multiprocess-0.70.15\n",
      "  Attempting uninstall: pathos\n",
      "    Found existing installation: pathos 0.3.1\n",
      "    Uninstalling pathos-0.3.1:\n",
      "      Successfully uninstalled pathos-0.3.1\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.184.0\n",
      "    Uninstalling sagemaker-2.184.0:\n",
      "      Successfully uninstalled sagemaker-2.184.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.3 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.2.2 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.15.0 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.0a7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.21.0 aiohttp-3.8.5 aiosignal-1.3.1 annotated-types-0.5.0 async-timeout-4.0.3 bitsandbytes-0.40.2 cmake-3.27.6 dataclasses-json-0.6.1 datasets-2.13.0 dill-0.3.6 dnspython-2.4.2 frozenlist-1.4.0 huggingface-hub-0.16.4 langchain-0.0.297 langsmith-0.0.41 lit-17.0.1 loguru-0.7.2 marshmallow-3.20.1 multidict-6.0.4 multiprocess-0.70.14 numexpr-2.8.7 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pathos-0.3.0 pinecone-client-2.2.4 pydantic-2.4.2 pydantic-core-2.10.1 pypdf-3.16.2 safetensors-0.3.3 sagemaker-2.188.0 sentence_transformers-2.2.2 sentencepiece-0.1.99 tenacity-8.2.3 tokenizers-0.14.0 torch-2.0.1 torchvision-0.15.2 transformers-4.34.0.dev0 triton-2.0.0 typing-extensions-4.8.0 typing-inspect-0.9.0 urllib3-1.26.17 xxhash-3.3.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b62ea-3dc3-4e1a-91c8-1359f112bd38",
   "metadata": {
    "tags": []
   },
   "source": [
    "The below is only needed if using bits and bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14c96bb5-a3be-45ce-b38b-95c2523a6a72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import nvidia\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cuda_install_dir = '/'.join(nvidia.__file__.split('/')[:-1]) + '/cuda_runtime/lib/'\n",
    "    os.environ['LD_LIBRARY_PATH'] =  cuda_install_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e1778-d2e5-41a4-b6db-c377cdbafba8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 02. Load Llama-2 7B chat in the notebook for experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa4d6d-7b9b-4b35-a270-c3fed44ce5e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, let's download the Llama-2-7b-chat-hf model from the Hugging Face Hub. Llama 2 models are gated, to get access follow the instructions [here](https://huggingface.co/meta-llama/Llama-2-7b-hf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b3b0694-aacd-4d7f-9792-ba416dcfdf24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LlamaTokenizer, LlamaForCausalLM, GenerationConfig,AutoModelForCausalLM\n",
    "import transformers\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\" #the model id in Hugging Face\n",
    "model_path = f\"./model/{model_id}\" #the local directory where the model will be saved\n",
    "\n",
    "access_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8f69641-86ac-4532-92d1-19b7ce4b46a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.90s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "if  not (os.path.exists(model_path)) or os.listdir(model_path and model_path) == []:\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=10.0)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, \n",
    "        token=access_token, \n",
    "        device_map=\"auto\", \n",
    "        do_sample=True, \n",
    "        use_safetensors=True, \n",
    "    #    quantization_config=quantization_config ,\n",
    "        torch_dtype=torch.float16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)\n",
    "\n",
    "    model.save_pretrained(save_directory=model_path, from_pt=True)\n",
    "    tokenizer.save_pretrained(save_directory=model_path, from_pt=True)\n",
    "else:\n",
    "    model = LlamaForCausalLM.from_pretrained(model_path,device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d7126-5d9c-4cc5-abd8-b0e3551dcc12",
   "metadata": {},
   "source": [
    "Check memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9521f112-fb50-4bb7-a0d9-d1c481429665",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 5.793186GB\n",
      "Memory reserved  5.806641GB\n",
      "Max memory reserved: 5.806641GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Memory allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"Memory reserved  %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"Max memory reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d8188-0bf0-44f4-93d9-b2e4ead0bc17",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 03. Simple question-answering using Llama 2 7B chat and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5875641-131d-4ebf-bcef-a50c2834b845",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that the model is available in memory, we can start using it to answer questions. The Llama-2 chat models expect the prompt to follow the below format:\n",
    "\n",
    "    \n",
    "\\<s>[INST] <\\<SYS\\>>\n",
    "\n",
    "{{ system_prompt }}\n",
    "\n",
    "\\<<SYS\\>>\n",
    "\n",
    "{{ user_message }} [/INST]\n",
    "\n",
    "   \n",
    "where\n",
    "- \\<s> - is the beginning of the sequence.\n",
    "- <\\<SYS>> - is the beginning of the system message.\n",
    "- \\<</SYS\\>> - is the end of the system message.\n",
    "- [INST] - is the beginning of the instructions\n",
    "- [/INST] - is the end of the instructions\n",
    "\n",
    "Let's create a recipe based on the above that will helps us define our prompts going forward. For that we will use [PromptTemplate](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/) from langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c4d5e11-2a1c-4981-a16d-a305325d0c34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"<s>[INST] <<SYS>>\\nYou are an assistant for question-answering tasks. You are helpful and friendly. Use the following pieces of retrieved context to answer the query. If you don't know the answer, you just say I don't know. Use three sentences maximum and keep the answer concise.\n",
    "<<SYS>>\\n\n",
    "{context}\\n\n",
    "{question} [/INST]\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(template=template, input_variables=['context','question'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d6a863-6d09-43a3-a21d-ec52de136bf6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we test the model on some questions without providing any context. For our tests, we will use questions about the UK Home Office activities in 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2c9fb93-b816-40a1-a97b-5726a96f400b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"How is UK home office driving down crime in 2023?\"\n",
    "question2= \"How are the nationality fees changing in 2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7500142a-8cc4-45dd-a0f0-e46abb7d0b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "    task='text-generation',\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=400,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3df8b60c-a237-46ad-9061-36bab130a263",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"According to the latest crime statistics released by the UK Home Office, crime in England and Wales has been steadily decreasing over the past few years, with a 7% drop in crime rates in 2023 compared to the previous year. This downward trend is largely attributed to the government's ongoing efforts to improve police resources, enhance criminal justice measures, and invest in community-based initiatives aimed at preventing crime. Additionally, the use of advanced technology, such as facial recognition software and predictive policing algorithms, has helped law enforcement agencies to more effectively combat crime.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0.7})\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "llm_chain.predict(context=\"\", question=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074dabc6-d77e-4a98-85dc-12514b6b0783",
   "metadata": {},
   "source": [
    "Although this answer is inline with what was asked it doesn't provide a lot of detail on the specific activities that took place in 2023. To improve it we pass some context. The below is an extract from  the [Home Office Annual Report and Accounts 2022-2023](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1185849/Home_Office_Annual_Report_and_Accounts_22-23.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39727de5-6eef-492c-969c-013a8fe64e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\"Overall crime, excluding fraud and computer crime which has only been counted since\n",
    "2016, is now down by 54% since 2010, with burglary down 55%, robbery down 77%,\n",
    "violence down 46%, theft down 47%, neighbourhood crime down 51%, and criminal\n",
    "damage down 72%. We are rolling out our ambitious programme, Operation Soteria, to\n",
    "transform rape investigations and prosecutions and have brought in a big package of\n",
    "measures for domestic abuse victims. Our new Fraud Strategy will mean this wicked crime\n",
    "is treated like the epidemic it is.\n",
    "We received the Third Volume of the Manchester Arena Inquiry report and published the\n",
    "Terrorism (Protection of Premises) draft Bill. This is also known as Martyn’s Law. It will\n",
    "place on public places a greater duty to protect their visitors.\n",
    "Our efforts to drive down crime have been boosted by recruiting 20,951 additional police\n",
    "officers by March 2023, exceeding our manifesto commitment to recruit an additional\n",
    "20,000 by this date. This brings the total number of police officers in England and Wales to\n",
    "149,566 in March 2023 – the highest on record against a previous peak of 146,030 in\n",
    "2010. Now the police need to ensure they focus on getting the basics right: the highest\n",
    "professional standards and a relentless focus on crime, not politically correct distractions.\n",
    "This means continuing to smash county lines gangs, using proven methods like stop and\n",
    "search, and deploying officers to high-crime areas. The Anti-Social Behaviour Action Plan\n",
    "reflects the fact that there is no such thing as petty crime and that it is easy for areas to\n",
    "slip into degeneracy and misery\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d60d4814-21bc-4448-a08a-0eb87025c219",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The UK Home Office is driving down crime in 2023 through various initiatives, including the rollout of Operation Soteria to transform rape investigations and prosecutions, a new Fraud Strategy to tackle the epidemic of fraud, and the recruitment of 20,951 additional police officers by March 2023, exceeding the manifesto commitment. The Home Office is also focusing on getting the basics right by ensuring police officers maintain the highest professional standards and focus on crime, rather than politically correct distractions. This includes continuing to disrupt county lines gangs, using proven methods like stop and search, and deploying officers to high-crime areas.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0.7})\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "llm_chain.predict(context=context, question=question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252fb6a-305b-43fc-9de5-dfba1b6387cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 04. RAG question answering with Llama 2 7B chat, LangChain and Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c762de8c-0a83-4d64-a4cd-f45f7684da69",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "In the above response, the model provides an answer with data from 2023 based on the context we provided. Next we want to scale this approach using __Retrieval Augmented Generation (RAG)__.\n",
    "With RAG, we will ingest external data into our knowledge base and augment the prompt by adding only the data that is relevant to the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc01627-1b6d-4366-9fc9-ab87661f3700",
   "metadata": {
    "tags": []
   },
   "source": [
    "First we download the external files we want to store in our knowledge base locally so than we can quickly iterate if needed. We will use reports published by the UK Home office in 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e840bf42-f473-469f-90ef-039bb67f6cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "files = [\n",
    "    \"https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1185849/Home_Office_Annual_Report_and_Accounts_22-23.pdf\",\n",
    "    \"https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1185949/2023-9-18_Equality_Impact_Assessment_for_Autumn_2023_fee_increases_FINAL.pdf\",\n",
    "]\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "for url in files:\n",
    "    file_path = os.path.join(\"data\", url.rpartition(\"/\")[2])\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1b4d5b-0cfb-422b-aa90-ed481b48194d",
   "metadata": {},
   "source": [
    "After that, we split files into documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc83614a-a646-4d4b-ae6f-8d5f6964c038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 50,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702ef35-4338-4aa4-9a4a-481a5db011dc",
   "metadata": {},
   "source": [
    "Next, we generate the embeddings for the documents. For that we will use the [bge-small-en](https://huggingface.co/BAAI/bge-small-en) model. We can do that easily in our notebook through the HuggingFaceBgeEmbeddings class in langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fb59413-1ae8-4213-bd96-434bfc218674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe3504-7bae-46f1-a0aa-360238e5cb0e",
   "metadata": {},
   "source": [
    "The dimension for our embeddings model is 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a183a3c-4e56-42a0-aa37-468f0a438977",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the embedding:  384\n"
     ]
    }
   ],
   "source": [
    "#optionally take a look at one of the embeddings\n",
    "sample_embedding = np.array(embeddings.embed_query(docs[0].page_content))\n",
    "print(\"Size of the embedding: \", sample_embedding.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3bd75-c8d8-4c25-afe0-25406249be72",
   "metadata": {},
   "source": [
    "We are now ready to ingest the embeddings into our vector store. In this notebook we will use [Pinecone](https://www.pinecone.io/), however you can replace the below code with that for the vector store of your choice.\n",
    "If you don't have a Pinecone account you can sign up for free to complete this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fdf57ca-368e-4275-9a7b-f23d6831abb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Pinecone API Key: ········\n",
      "Pinecone Environment: ········\n"
     ]
    }
   ],
   "source": [
    "#enter your Pinecone keys\n",
    "import getpass\n",
    "os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\n",
    "os.environ[\"PINECONE_ENV\"] = getpass.getpass(\"Pinecone Environment:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3167c11c-3900-4fb2-88e9-27dabe57186d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initialize Pinecone\n",
    "import pinecone\n",
    "pinecone.init(\n",
    "    api_key = os.environ[\"PINECONE_API_KEY\"],\n",
    "    environment = os.environ[\"PINECONE_ENV\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead9204-a1c6-4e9c-9b83-6608a00e7a54",
   "metadata": {},
   "source": [
    "In Pinecone, we create a new vector search index and ingest the embeddings we created in the previous step. The size of the index is the dimension of our embeddings model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2ffc6e6-f9b6-45f4-a87c-473f24098368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check if index already exists, if not we create it\n",
    "index_name = \"qa-index\"\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric='cosine'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7089eae7-f173-476e-9004-535893cfd2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#insert the embeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "vector_store = Pinecone.from_documents(docs, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439c8ed5-dcb2-45ab-9737-c75043848e0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's do a quick test to see if the similarity search is working well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff8f1253-5cb9-4225-a7fe-31e28fde496f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 | Annual Report  & Accounts  2022 -2023  The Home Office will continue to work closely with the Department for Levelling Up,  \n",
      "Housing & Communities to co -ordinate its contributions to the levelling up missions. The \n",
      "Department’s work ‘to reduce homicide, serious violence and neighbourhood crime, \n",
      "focused on the worst affected areas’ is essential in supporting this project.  By reduc ing \n",
      "crime, particularly violent crime, it will attract investment, create employment opportunities,\n"
     ]
    }
   ],
   "source": [
    "docs = vector_store.similarity_search(question)\n",
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23974b7-71dc-4024-9b60-dcbe42dace73",
   "metadata": {
    "tags": []
   },
   "source": [
    "We have Llama-2 chat model in memory and the embeddings inserted in our Pinecone index. To improve the responses of the Llama 2 chat model we bring it alltogether and implement the RAG architecture easily with the Langchain [RetrievalQA](https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa). RetrievalQA augments our initial prompt with the most similar documents from the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3d4c5f9-5c2d-4272-83b7-f106486da0d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm_qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                     chain_type='stuff',\n",
    "                                     retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "                                     return_source_documents=True,\n",
    "                                     chain_type_kwargs={\"prompt\": prompt_template})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe4e48a-e4e2-4d92-a349-f1df63121b20",
   "metadata": {},
   "source": [
    "And that's it! Let's ask the model again to see if we will get 2023 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13d80570-d172-42d4-aa89-e3eceb115111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_response(llm_response):\n",
    "    temp = [textwrap.fill(line, width=100) for line in llm_response['result'].split('\\n')]\n",
    "    response = '\\n'.join(temp)\n",
    "    print(f\"{llm_response['query']}\\n \\n{response}'\\n \\n Source Documents:\")\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52c7fdf0-8665-431b-a2a9-c6ca97836fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is UK home office driving down crime in 2023?\n",
      " \n",
      "The UK Home Office is driving down crime in 2023 through various strategies and initiatives,\n",
      "including:\n",
      "1. Delivering two phases of the long-term national behavior change campaign 'Enough', which has\n",
      "reached millions across England and Wales.\n",
      "2. Working closely with the Department for Levelling Up, Housing & Communities to coordinate\n",
      "contributions to the levelling up missions, with a focus on reducing homicide, serious violence, and\n",
      "neighborhood crime in the worst affected areas.\n",
      "3. Using police recorded crime data collected by the Home Office to monitor progress and make\n",
      "informed decisions. According to the latest figures for year to March 2023, levels of homicide are\n",
      "15% below the December 2019 (pre-pandemic) baseline.'\n",
      " \n",
      " Source Documents:\n",
      "{'page': 36.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n",
      "{'page': 21.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n",
      "{'page': 30.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print_response(llm_qa_chain(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb6772-54bb-40e4-bd67-fbdc6c896644",
   "metadata": {
    "tags": []
   },
   "source": [
    "The model returns a more informed response with details from 2023 and the pages in the documents from where it acquired the information. \n",
    "\n",
    "Let's try another question. The answer to this one is in a different document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00db9ed9-4124-41ad-8750-69005b2fbd94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are the nationality fees changing in 2023\n",
      " \n",
      "According to the document, the department is proposing a range of changes to immigration and\n",
      "nationality fees in Autumn 2023. These changes include a 20% increase to the Leave to Remain fee,\n",
      "which will be laid in further Regulations in due course. Additionally, there was a slight uplift in\n",
      "appeals allowed throughout 2022, and Returns (voluntary and enforced) for the first three quarters\n",
      "of 2022 were up compared to the same period of 2021.'\n",
      " \n",
      " Source Documents:\n",
      "{'page': 1.0, 'source': 'data/2023-9-18_Equality_Impact_Assessment_for_Autumn_2023_fee_increases_FINAL.pdf'}\n",
      "{'page': 1.0, 'source': 'data/2023-9-18_Equality_Impact_Assessment_for_Autumn_2023_fee_increases_FINAL.pdf'}\n",
      "{'page': 20.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print_response(llm_qa_chain(question2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c6aafb-8dee-4dfc-b50e-0a4c3a384f7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can continue our experimentation with more files, different model parameters and different questions. Once we have sufficiet confidence in our approach, \n",
    "we can deploy our models to Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13e2e9-4257-4312-9fe2-e4dd581cd238",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 04. Supercharge your applications with GenAI by deploying your models to Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6df67-2b16-454e-af38-885eac2f5cab",
   "metadata": {
    "tags": []
   },
   "source": [
    "First we import the required libraries, and retrieve the IAM role and session we will use for deployment.  To deploy a model to a SageMaker endpoint, we first need to compress the model artifacts and upload the tar.gz file to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "faa5920f-fbc1-4fae-9952-df84ef211acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = sagemaker.Session().boto_region_name\n",
    "bucket = sess.default_bucket() # Set a default S3 bucket\n",
    "prefix = 'qa-rag-models-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f540b9f6-4422-46d8-877b-9effd162b53c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_model_location = f\"s3://{bucket}/{prefix}/llama-2-7B-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea17fdee-269e-47f6-acb9-f7972258a5e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_path = sagemaker.s3.S3Uploader.upload(model_path, pretrained_model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "186dcc74-8b08-4407-9293-c986178c58f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "djl_properties_filename = \"serving.properties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "450f48fa-80f7-472b-b3fa-2fed59e7600b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile {djl_properties_filename}\n",
    "engine = MPI\n",
    "option.tensor_parallel_degree = 1\n",
    "option.rolling_batch = auto\n",
    "option.max_rolling_batch_size = 64\n",
    "option.model_loading_timeout = 3600\n",
    "option.paged_attention = true\n",
    "option.trust_remote_code = true\n",
    "option.dtype = fp16\n",
    "option.rolling_batch=lmi-dist\n",
    "option.max_rolling_batch_prefill_tokens=1560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8dbac860-dba1-428b-a1d8-78d5476b86b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!echo -n \"option.s3url = $pretrained_model_location\" >> {djl_properties_filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c7856a2f-ed96-46b8-ac22-2bdad4930739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelfile_base_name = f\"local-{model_id.replace('/', '-')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "be5a6648-204c-4ddc-9699-6f453e40f8bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local-meta-llama-Llama-2-7b-chat-hf/\n",
      "local-meta-llama-Llama-2-7b-chat-hf/serving.properties\n"
     ]
    }
   ],
   "source": [
    "!mkdir {modelfile_base_name}\n",
    "!mv serving.properties {modelfile_base_name}/\n",
    "!tar czvf {modelfile_base_name}.tar.gz {modelfile_base_name}/\n",
    "!rm -rf {modelfile_base_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c9e8fef6-ff1c-4318-bea8-8aab69d07cad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x root/root         0 2023-10-03 05:21 local-meta-llama-Llama-2-7b-chat-hf/\n",
      "-rw-r--r-- root/root       390 2023-10-03 05:18 local-meta-llama-Llama-2-7b-chat-hf/serving.properties\n"
     ]
    }
   ],
   "source": [
    "# list out the contents of the tar gz file for validation\n",
    "!tar -ztvf {modelfile_base_name}.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ec63ea6f-83dc-4e25-931f-a44af164e8bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", region=region, version=\"0.23.0\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c75196f0-46b5-47a8-b246-f1fe56233c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-east-1-811828458885/large-model-lmi/artifacts/local-meta-llama-Llama-2-7b-chat-hf.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Upload file and instantiate a new SageMaker Model\n",
    "s3_code_prefix = \"large-model-lmi/artifacts\"\n",
    "\n",
    "code_artifact = sess.upload_data(f\"{modelfile_base_name}.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "531a0809-b888-40dc-b053-23b20090b954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama2_model_name = sagemaker.utils.name_from_base(\n",
    "    f\"{model_id.replace('/', '-')}\"\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=code_artifact,\n",
    "    role=role,\n",
    "    name=llama2_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "52ce3e4f-b0fe-44d9-afe8-477ad042a5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!"
     ]
    }
   ],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "endpoint_name = f\"ep-{llama2_model_name}\"\n",
    "\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=900,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "95a750c8-9150-4040-8b59-4763d1827dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name to use ---> ep-meta-llama-Llama-2-7b-chat-hf-2023-10-03-05-26-04-539\n"
     ]
    }
   ],
   "source": [
    "ep_name = model.endpoint_name\n",
    "print(f\"Endpoint name to use ---> {ep_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5ad5f953-274e-4ede-be23-2e3f0e3caea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=ep_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4796940c-6fa5-4114-abd8-36009fa2a370",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': '\\n\\nThe current president of Brazil is Jair Bolsonaro. He was inaugurated on January 1, 2019, and is serving a four-year term as the 38th president of Brazil. Prior to his'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(\n",
    "    {\n",
    "        \"inputs\": \"Who is the president of Brazil?\",\n",
    "        \"parameters\": {\"temperature\": 0.1, \"max_new_tokens\": 50}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f81c1-be49-4789-8dd7-92e1590cd818",
   "metadata": {},
   "source": [
    "## 05. Run LangChain Inference using SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8faab706-c63b-49fe-a3ac-e0718aab3e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.llms import SagemakerEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "889ba435-b220-4ce4-a662-f5d03d32c235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        body = {\"inputs\": prompt, \"parameters\": model_kwargs}\n",
    "        input_str = json.dumps(body)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"generated_text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "05d91047-1aad-4c07-8f84-afe677b3e3e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content_handler = ContentHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "df9aeb46-aeb5-427d-895c-25271b82822e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert your local LLM into SageMaker endpoint LLM\n",
    "llm_sm_ep = SagemakerEndpoint(\n",
    "    endpoint_name=ep_name, # <--- Your endpoint name\n",
    "    region_name=\"us-east-1\",\n",
    "    model_kwargs={\"temperature\": 0.05, \"max_new_tokens\": 512},\n",
    "    content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ee6d250c-c9cf-4d43-8f8b-075b13cb3d02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_qa_smep_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm_sm_ep,\n",
    "    chain_type='stuff',\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1390fab0-e427-4264-8cd5-5711f8f762ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is UK home office driving down crime in 2023?\n",
      " \n",
      "The UK Home Office is driving down crime in 2023 through various strategies and initiatives,\n",
      "including:\n",
      "1. Implementing 35% of the commitments in the 2018 and 2020 crime reduction strategies, such as\n",
      "delivering two phases of the long-term national behaviour change campaign 'Enough' which has reached\n",
      "millions across England and Wales.\n",
      "2. Working closely with the Department for Levelling Up, Housing & Communities to coordinate\n",
      "contributions to the levelling up missions, with a focus on reducing homicide, serious violence, and\n",
      "neighbourhood crime in the worst affected areas.\n",
      "3. Using police recorded crime data collected by the Home Office to monitor progress and make steady\n",
      "progress towards delivering this priority, with levels of homicide being 15% below the December 2019\n",
      "(pre-pandemic) baseline for the year to March 2023.'\n",
      " \n",
      " Source Documents:\n",
      "{'page': 36.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n",
      "{'page': 21.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n",
      "{'page': 30.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print_response(llm_qa_smep_chain(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "be62da26-9769-48ff-81b3-de5bb1d01703",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are the nationality fees changing in 2023\n",
      " \n",
      "According to the document, the department is proposing a range of changes to immigration and\n",
      "nationality fees in Autumn 2023, with the objective of increasing the level of income generated from\n",
      "those fees to mitigate wider costs. Specifically, the fees for Leave to Remain applications are\n",
      "proposed to increase by 20%, and there will be a slight uplift in appeals allowed throughout 2022.'\n",
      " \n",
      " Source Documents:\n",
      "{'page': 1.0, 'source': 'data/2023-9-18_Equality_Impact_Assessment_for_Autumn_2023_fee_increases_FINAL.pdf'}\n",
      "{'page': 1.0, 'source': 'data/2023-9-18_Equality_Impact_Assessment_for_Autumn_2023_fee_increases_FINAL.pdf'}\n",
      "{'page': 20.0, 'source': 'data/Home_Office_Annual_Report_and_Accounts_22-23.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print_response(llm_qa_smep_chain(question2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7594e8ba-acf3-49d7-b707-36403286d4de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g5.12xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
